{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9578585,
          "sourceType": "datasetVersion",
          "datasetId": 5839733
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Desafio 3"
      ],
      "metadata": {
        "id": "l1kyKDUtlLNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reutilización de notebooks para evaluar otro archivo de texto"
      ],
      "metadata": {
        "id": "_-y_2zJylLNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout, Dense, Bidirectional\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:23.768176Z",
          "iopub.execute_input": "2024-10-08T22:33:23.769125Z",
          "iopub.status.idle": "2024-10-08T22:33:39.740554Z",
          "shell.execute_reply.started": "2024-10-08T22:33:23.769077Z",
          "shell.execute_reply": "2024-10-08T22:33:39.739364Z"
        },
        "trusted": true,
        "id": "Xmy-t9NIlLNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:39.742087Z",
          "iopub.execute_input": "2024-10-08T22:33:39.742780Z",
          "iopub.status.idle": "2024-10-08T22:33:39.962872Z",
          "shell.execute_reply.started": "2024-10-08T22:33:39.742735Z",
          "shell.execute_reply": "2024-10-08T22:33:39.961674Z"
        },
        "trusted": true,
        "id": "05JwHnWxlLNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usaremos las utilidades de procesamiento de textos y secuencias de Keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # equivalente a ltokenizer de nltk\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence # equivalente a word_tokenize de nltk\n",
        "from tensorflow.keras.utils import pad_sequences # se utilizará para padding"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:39.966245Z",
          "iopub.execute_input": "2024-10-08T22:33:39.967164Z",
          "iopub.status.idle": "2024-10-08T22:33:39.978215Z",
          "shell.execute_reply.started": "2024-10-08T22:33:39.967105Z",
          "shell.execute_reply": "2024-10-08T22:33:39.976808Z"
        },
        "trusted": true,
        "id": "ko7xLR5tlLNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de archivo de texto para analizar"
      ],
      "metadata": {
        "id": "2IJIFW0GlLNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cada verso lo guardamos en una lista\n",
        "text = []\n",
        "filename = \"./data/74376.txt\"\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        text.append(line)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:39.980309Z",
          "iopub.execute_input": "2024-10-08T22:33:39.980868Z",
          "iopub.status.idle": "2024-10-08T22:33:40.011079Z",
          "shell.execute_reply.started": "2024-10-08T22:33:39.980809Z",
          "shell.execute_reply": "2024-10-08T22:33:40.009769Z"
        },
        "trusted": true,
        "id": "YLLh2agNlLNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento"
      ],
      "metadata": {
        "id": "nNNHnCYOlLNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# segmentamos el texto con la utilidad de Keras\n",
        "segmented_sentences = [text_to_word_sequence(sentence) for sentence in text]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.012628Z",
          "iopub.execute_input": "2024-10-08T22:33:40.013048Z",
          "iopub.status.idle": "2024-10-08T22:33:40.067876Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.013005Z",
          "shell.execute_reply": "2024-10-08T22:33:40.066248Z"
        },
        "trusted": true,
        "id": "kFVdv7lLlLNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculamos la longitud de cada secuencia\n",
        "length_sentences = [len(sentence) for sentence in segmented_sentences]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.069526Z",
          "iopub.execute_input": "2024-10-08T22:33:40.069930Z",
          "iopub.status.idle": "2024-10-08T22:33:40.081460Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.069890Z",
          "shell.execute_reply": "2024-10-08T22:33:40.080047Z"
        },
        "trusted": true,
        "id": "v-QPQhDQlLNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# podemos ver su distribución\n",
        "plt.hist(length_sentences,bins=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.082909Z",
          "iopub.execute_input": "2024-10-08T22:33:40.083293Z",
          "iopub.status.idle": "2024-10-08T22:33:40.382712Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.083253Z",
          "shell.execute_reply": "2024-10-08T22:33:40.381488Z"
        },
        "trusted": true,
        "id": "shESK00TlLNj",
        "outputId": "f5fe2401-04a1-4811-ce73-52d3cc55c5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a partir de la distribución de longitudes de secuencias elegimos algún criterio\n",
        "# para determinar el máximo tamaño de contexto. En este caso es un percentil, pero\n",
        "# otros criterios también pueden ser válidos con la justificación adecuada.\n",
        "\n",
        "# el -1 es porque el último token será el target\n",
        "max_context_size = int(np.percentile(length_sentences, 90)-1)\n",
        "# max_context_size = int(np.ceil(np.mean(length_sentences))) # criterio de media\n",
        "# max_context_size = int(np.ceil(np.median(length_sentences))) # criterio de mediana\n",
        "print(f'max_context_size: {max_context_size}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.384052Z",
          "iopub.execute_input": "2024-10-08T22:33:40.384440Z",
          "iopub.status.idle": "2024-10-08T22:33:40.394746Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.384401Z",
          "shell.execute_reply": "2024-10-08T22:33:40.393334Z"
        },
        "trusted": true,
        "id": "e-luWOzqlLNj",
        "outputId": "963373e7-cf92-4c8c-8ce5-b4b125f7a333"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "max_context_size: 13\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizar"
      ],
      "metadata": {
        "id": "GFkydDkdlLNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok = Tokenizer()\n",
        "\n",
        "# El tokenizer \"aprende\" las palabras que se usaran\n",
        "# Se construye (fit) una vez por proyecto, se aplica N veces (tal cual un encoder)\n",
        "# El token 0 es reservado y no es asignado. Se utiliza para designar a palabras\n",
        "# fuera del vocabulario aprendido\n",
        "tok.fit_on_texts(segmented_sentences)\n",
        "\n",
        "# Convertimos las palabras a números\n",
        "# entran palabras -> salen números\n",
        "tokenized_sentences = tok.texts_to_sequences(segmented_sentences)\n",
        "tokenized_sentences"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.400477Z",
          "iopub.execute_input": "2024-10-08T22:33:40.400924Z",
          "iopub.status.idle": "2024-10-08T22:33:40.737388Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.400881Z",
          "shell.execute_reply": "2024-10-08T22:33:40.736021Z"
        },
        "trusted": true,
        "id": "XX0NShvBlLNk",
        "outputId": "d04eb8cb-da35-4b95-a648-0fb3f6edaf36"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[[2337, 52, 122, 371, 2, 1, 19, 3, 159],\n [],\n [14, 371, 6, 15, 1, 77, 2, 859, 1249, 5, 1, 254, 186, 3],\n [93, 64, 210, 2, 1, 110, 35, 61, 1015, 3, 10, 438, 61, 1250],\n [1625, 50, 53, 347, 9, 130, 9, 573, 12, 1251, 77, 9, 187, 1, 168],\n [2, 1, 52, 122, 275, 1016, 10, 14, 371, 12, 860],\n [35, 479, 122, 514, 49, 50, 20, 17, 574, 5, 1, 254, 186],\n [50, 65, 29, 4, 1017, 1, 400, 2, 1, 348, 103, 50, 20, 574],\n [255, 401, 14, 371],\n [],\n [2338, 1, 19, 3, 159],\n [],\n [2339, 1018, 1019],\n [],\n [2340, 1252, 2341, 745, 2342, 371, 2343],\n [],\n [243, 2344],\n [],\n [746, 2345, 402, 2346, 1, 144, 2, 402, 644, 1253],\n [],\n [2347,\n  2348,\n  2349,\n  3,\n  1,\n  860,\n  861,\n  2350,\n  2351,\n  35,\n  2352,\n  479,\n  2353,\n  2354,\n  14,\n  1626,\n  41,\n  1254,\n  24,\n  862,\n  2355,\n  62,\n  575,\n  21,\n  1,\n  2356,\n  349],\n [],\n [],\n [645, 2, 1, 52, 122, 371, 1, 19, 3, 159],\n [],\n [],\n [],\n [],\n [],\n [],\n [1, 19, 3, 159],\n [],\n [1, 144, 2, 402, 644],\n [402, 1255],\n [],\n [244],\n [],\n [1, 2357, 2358, 1256],\n [136, 1257],\n [],\n [1, 2359, 144, 644],\n [2360],\n [],\n [],\n [],\n [],\n [1],\n [19],\n [3],\n [159],\n [],\n [21, 1018, 1019],\n [],\n [1627, 863],\n [],\n [244],\n [],\n [1, 144, 2, 402, 644],\n [402, 2361, 1255],\n [],\n [222, 1628, 21, 1, 144, 2, 402],\n [222, 1628, 3, 1258, 21, 1018, 1019],\n [25, 1259, 2362, 1629, 2363, 1253],\n [291, 863, 2364, 1258],\n [2365, 1630, 1631, 2366],\n [],\n [244],\n [],\n [2367, 3, 1020, 21, 1, 144, 2, 402, 644],\n [402, 1255, 515, 403, 7],\n [],\n [],\n [],\n [],\n [4],\n [864, 1632, 1633],\n [4, 865, 99, 5, 96],\n [2368],\n [1, 1634, 2, 14, 439],\n [6, 646],\n [],\n [],\n [],\n [],\n [2369],\n [],\n [],\n [1021],\n [],\n [57, 1, 19, 3, 31, 325, 292],\n [1022, 1, 19, 3, 1, 23, 2, 1, 18, 2370],\n [866, 404, 5, 67, 2371],\n [1023, 1, 223, 2, 169, 67, 2372],\n [1635, 647, 96, 235, 2373],\n [1636, 1, 223, 2, 88, 2374],\n [1637, 1, 182, 2, 104, 2375],\n [1638, 1, 211, 2, 100, 5, 169, 67, 2376],\n [],\n [],\n [],\n [],\n [1639, 2, 1260],\n [],\n [],\n [2377, 1021],\n [],\n [350, 2, 7, 1261, 3, 648, 2378],\n [350, 2, 7, 867, 2379],\n [350, 2, 747, 326, 2380],\n [350, 2, 7, 1640, 326, 2381],\n [],\n [],\n [],\n [],\n [2382, 868],\n [],\n [],\n [1, 101, 440, 2383, 2, 14, 439, 74, 2384, 11, 869, 255],\n [32, 2385, 2, 1641, 3, 236, 649, 5, 1, 144, 169],\n [19, 5, 1, 1642, 2, 2386, 2, 1, 198, 1253, 870, 1019, 1627, 58, 5],\n [145, 24, 7, 2387, 1643, 3, 2388, 480, 3, 1, 1262],\n [1644, 170, 15, 1, 644, 29, 59, 62, 5, 28, 1645, 1],\n [869, 1263, 576, 1, 2389, 372, 11, 309, 11, 1, 212, 2],\n [1, 1024, 650, 11, 38, 2390, 44, 12, 293, 2391, 10, 1, 22, 2],\n [1, 169, 19, 870, 2392, 2393, 481, 2, 14, 37],\n [59, 1646],\n [],\n [],\n [],\n [],\n [1264, 868],\n [],\n [],\n [7, 291, 1265, 2394, 7, 2395, 405, 15, 2396, 8, 14],\n [137, 439, 6, 7, 1647, 2, 1, 406, 871, 651, 3, 2397, 2, 160],\n [748, 30, 2398, 4, 864, 1632, 1633, 6, 441, 577, 5],\n [1, 2399, 24, 516, 1025, 870, 3, 864, 2400, 2401, 2402, 872],\n [8, 99, 2403, 104, 4, 482, 3, 1266, 2404, 16],\n [5, 516, 1645, 2405, 2406, 2407, 483, 38, 74, 1026, 4, 1267],\n [3, 82, 1648, 1, 146, 55, 1, 644, 10, 1, 91, 2408],\n [2409, 652, 2, 2410, 62, 484, 16, 57, 2411, 4, 236],\n [2412, 327, 4, 2413, 72, 1025],\n [],\n [9, 105, 13, 32, 1649, 407, 16, 125, 1639, 25, 1, 1025, 865],\n [2414, 3, 2415, 2416, 37, 62, 171, 1, 19, 16],\n [2417, 3, 1027, 1, 149, 2, 43, 1650, 43, 1025, 57, 873, 1268],\n [105, 13, 1, 101, 4, 653, 1, 874, 2418, 2, 1651],\n [1269, 2, 1, 1652, 2, 864, 2419, 1653, 2420, 3, 864, 2421, 1653, 2422],\n [],\n [3, 1, 19, 89, 5, 30, 96, 22, 6, 7, 1270, 1028],\n [160, 29, 578, 5, 1271, 9, 1, 517, 3, 1654, 1029],\n [2, 516, 2423, 6, 1655, 1272, 48, 30, 1656, 1, 1657, 2424, 3],\n [2425, 2, 30, 2426, 29, 373, 94, 7, 749, 2, 30],\n [746, 1658, 2427, 48, 1659, 161, 3, 1660, 10, 23],\n [3, 654, 2, 51, 66, 518, 1, 2428, 2, 1, 149, 485, 5],\n [14, 439, 1, 579, 580, 24, 1, 406, 655, 2, 1, 2429],\n [651, 3, 1030, 2, 160, 748, 5, 1028, 4, 1661, 1, 23, 2],\n [1, 18, 65, 1662],\n [],\n [],\n [],\n [],\n [1264, 868, 4, 291, 863],\n [],\n [],\n [1, 91, 863, 1273, 68, 1262, 1274, 2430, 2, 1, 440],\n [869, 2431, 1, 101, 1275, 2, 1, 439, 1, 581, 1275],\n [6, 1016, 15, 1, 101, 106, 1276, 107, 2432, 10, 68],\n [480, 24, 1, 1264, 875, 4, 1, 169, 19],\n [750, 351, 45, 2, 1267],\n [],\n [1, 2433, 53, 442, 13, 1277, 7, 650, 4, 656, 28, 579],\n [8, 1, 96, 138, 2, 408, 485, 5, 14, 439, 6, 17, 33],\n [2434, 11, 9, 41, 2435, 154, 519, 3, 28, 443, 4, 328, 8, 1],\n [96, 520, 2, 16, 1, 439, 6, 32, 1663, 37, 17, 59],\n [155, 751, 5, 1, 224],\n [],\n [2436, 1664],\n [],\n [136, 1257, 582, 1631, 1258],\n [],\n [],\n [],\n [],\n [1, 19, 3, 31, 325],\n [],\n [],\n [],\n [],\n [57],\n [],\n [1, 19, 3, 31, 325],\n [],\n [],\n [27, 20, 1665, 4, 583, 35, 1, 19, 24, 32, 2437, 329, 11],\n [84, 80, 225, 3, 1666, 12, 80, 225, 3, 1278, 8],\n [16, 226, 227, 93, 6, 486, 1, 325, 62, 21, 1, 172],\n [18, 2, 69, 521, 28, 444, 162, 182, 28, 1031],\n [5, 1279, 4, 522, 1032, 3, 1033, 28, 90, 5, 1, 213, 2],\n [294, 3, 100, 1667, 5, 2438, 752, 2, 1668],\n [131, 3, 2439, 6, 24, 72, 1669, 11, 43, 8, 27, 1670],\n [1, 22, 2, 1, 19, 3, 1671, 33, 199, 1, 753, 2, 1, 1672],\n [200, 4, 13, 1034, 47, 1, 310, 3, 2440, 1278, 1280, 15, 28, 66],\n [18, 8, 71, 1, 409, 487, 15, 25, 2, 30, 42, 54, 64],\n [352, 15, 69, 256, 6, 657, 3, 2441, 1281, 46, 9, 2442],\n [69, 2443, 25, 8, 159, 37, 1673, 15, 89, 6, 311],\n [55, 1, 2444, 2, 1, 19, 35, 1, 2445, 2, 30, 374, 876],\n [25, 30, 523, 651, 2, 89, 9, 2446, 4, 658, 55, 1, 136],\n [1035, 139, 1674, 4, 30, 374, 445, 245, 2447, 3],\n [2448, 20, 35, 40, 56, 21, 201, 353, 4, 1, 163, 90, 2, 25, 1],\n [1282, 126, 92, 9, 78, 63, 159, 21, 54, 1283, 13, 353, 4, 89],\n [3, 5, 1, 445, 446, 139, 237, 524, 2449, 11, 81, 11, 1],\n [19, 15, 11, 2450, 2451, 330, 2452, 447, 6, 448, 40, 1284],\n [6, 1036, 7, 2453, 1251, 2454, 116],\n [],\n [1675, 27, 29, 5, 86, 1, 659, 2, 7, 136, 654, 5, 67],\n [9, 6, 584, 170, 4, 238, 1, 2455, 12, 31, 408],\n [1037, 480, 5, 1, 19, 1285, 3, 754, 65, 13],\n [2456, 35, 11, 1, 1286, 877, 2, 246, 525, 35, 1],\n [2457, 1676, 2458, 3, 35, 1, 310, 312, 2459, 5, 95],\n [2460, 14, 6, 1, 660, 46, 16, 9, 6, 354, 2461, 4, 878],\n [19, 480, 9, 6, 11, 1677, 4, 585, 2, 1, 1678, 12, 1],\n [1287, 11, 276, 661, 1, 1679, 331, 34, 5, 1, 277],\n [3, 375, 2, 67, 6, 11, 81, 7, 1038, 2, 1, 662, 31],\n [1680, 3, 11, 81, 32, 488, 4, 1039, 1, 200, 2, 1, 136, 159],\n [8, 6, 2462, 11, 20, 480, 5, 449, 2, 526, 3, 1040],\n [],\n [9, 6, 4, 14, 82, 8, 57, 584, 1041, 410, 104, 1, 488, 4],\n [585, 47, 1288, 53, 13, 879, 1, 1681, 1682, 5, 1, 355, 2],\n [278, 480, 5, 159, 63, 27, 586, 14, 1681, 1682, 10, 1],\n [228, 2463, 2, 1042, 49, 27, 63, 9, 65, 1289, 30, 411, 372],\n [9, 65, 1290, 4, 13, 32, 663, 16, 2464, 56, 24, 1, 295, 2465],\n [755, 2, 2466, 450, 10, 246, 756, 9, 65, 587, 11],\n [145, 3, 2467, 2, 1, 147, 31, 489, 3, 5, 30, 44, 228],\n [757, 35, 664, 11, 880, 881, 227, 82, 1041, 332, 1, 758, 882],\n [2, 1, 31, 654, 3, 1291, 665, 4, 1, 19, 4, 183, 47],\n [1683, 9, 490, 2, 488, 4, 311, 89, 5, 759, 3, 296, 9, 6, 412],\n [666, 4, 2468, 1, 147, 667, 57, 123, 15, 1, 93, 145, 1684],\n [2469, 4, 40, 279, 257, 5, 1, 760, 19, 2470, 16],\n [1685, 187, 1, 668, 2, 491, 2471, 49, 1, 150, 2, 8],\n [4, 662, 31, 73, 1043, 27, 123, 13, 297, 4, 2472, 1],\n [138, 11, 309, 669, 64, 96, 2473],\n [],\n [57, 92, 61, 2474, 15, 17, 2475, 35, 761, 46, 1, 31, 480],\n [5, 202, 118, 57, 123, 1269, 20, 2476, 33, 247, 8, 39, 126, 1292],\n [53, 522, 1, 224, 8, 229, 101, 4, 86, 1, 40, 8, 2477],\n [3, 102, 1686, 25, 236, 6, 1, 333, 1687, 588, 2],\n [173, 1688, 5, 1, 111, 877, 8, 29, 1044, 1, 376],\n [2, 127, 34, 7, 1293, 3, 2478, 1294, 1, 90, 2, 7, 110, 1689],\n [1690, 11, 1, 356, 2, 492, 2, 1293, 1691, 670, 4],\n [527, 14, 1690, 2, 1692, 3, 1295, 140, 2, 883, 3],\n [528, 80, 25, 30, 210, 102, 11, 4, 30, 2479, 2480],\n [14, 224, 6, 17, 81, 44, 79, 7, 671, 280, 5, 160, 2, 30, 93],\n [357, 882, 9, 1693, 334, 1, 1045, 2481, 2, 118, 151, 258],\n [40, 63, 298, 328, 36, 37, 59, 7, 762, 5, 25, 100, 33],\n [1295, 33, 1296, 33, 358, 55, 9, 1, 884, 2, 1, 313],\n [6, 214, 295, 102, 11, 4, 30, 162, 259, 885, 2482],\n [20, 2483, 45, 3, 1694, 94, 11, 49, 38, 74, 451, 56, 314, 34, 7],\n [886, 2484, 1695, 6, 2485, 1696, 48, 1697, 24, 1, 413],\n [2, 1, 313, 752, 2, 258, 20, 1698, 10, 2486, 2487],\n [3, 1046, 1, 1047, 15, 1, 672, 2, 127, 6, 1297],\n [2488, 3, 1699, 3, 51, 588, 4, 23, 62, 17, 56],\n [2489, 26, 2490, 170, 102, 69, 589, 3, 2491],\n [149, 3, 226, 1, 93, 2492, 132, 1, 1700, 1701],\n [83, 5, 69, 127, 20, 2493, 2494, 8, 14, 762],\n [125, 17, 2495, 67, 5, 68, 64, 79, 7, 673, 3, 1048],\n [1702, 6, 2496],\n [],\n [248, 2, 1, 1703, 188, 590, 1, 414, 3, 591, 188],\n [118, 2, 227, 126, 20, 245, 592, 203, 260, 248, 56, 40, 204, 12, 35, 93],\n [440, 1298, 4, 183, 7, 106, 70, 1, 414, 41, 452],\n [1, 453, 5, 16, 74, 281, 34, 12, 94, 16, 74, 2497],\n [25, 1, 279, 259, 2, 333, 230, 1, 674, 2498, 41],\n [15, 1, 93, 145, 62, 5, 1, 887, 1, 876, 2, 1, 414, 74],\n [1299, 454, 164, 10, 1, 2499, 2, 1, 1049, 1, 1300, 3],\n [326, 2, 1, 493, 3, 1, 2500, 2, 1, 888, 174, 2, 1704],\n [7, 2501, 3, 2502, 1, 887, 10, 1705, 355, 1, 147, 377],\n [2, 299, 2503, 41, 675, 5, 30, 1706, 761, 24, 1],\n [1707, 2, 1, 889, 3, 1, 1050, 2, 1708, 4, 1, 214, 2, 2504, 3],\n [2505, 2, 2506, 1, 527, 2, 2507, 2, 2508, 2, 1709, 2, 455],\n [97, 2, 414, 1301, 102, 2, 1302, 2509, 2, 2510, 2511],\n [2512, 119, 41, 1254, 5, 1, 529, 591, 5, 1051, 16],\n [74, 1052, 593, 4, 2513, 3, 261, 670, 2, 591],\n [2514, 1, 494, 333, 377, 2515, 1710, 24, 1],\n [492, 34, 1, 1303, 2, 1, 763, 97, 1304, 1, 1711, 2516, 41],\n [1053, 311, 4, 77, 17, 56, 14, 26, 452, 315, 1712, 2, 1],\n [414, 120, 28, 66, 1305, 5, 1, 22, 1, 42, 11, 38, 594, 5],\n [2517, 3, 890, 74, 764, 1713, 48, 1, 2518, 2, 1],\n [1714, 189, 9, 41, 7, 148, 2, 529, 3, 276, 1715],\n [102, 4, 1, 138, 2, 359, 1306],\n [],\n [27, 316, 1716, 1, 415, 2, 114, 3, 2, 372, 455],\n [282, 5, 14, 595, 2, 23, 190, 5, 752, 2, 131, 3, 2],\n [526, 3, 5, 1, 165, 2, 891, 2, 2519, 4, 60],\n [84, 4, 1054, 84, 5, 1, 110, 36, 41, 335, 84],\n [16, 416, 417, 4, 13, 215, 3, 7, 231, 249, 8, 216, 1712],\n [2, 1, 414, 125, 60, 28, 66, 145, 2520, 3, 5, 406, 655],\n [10, 236, 1717, 16, 892, 676, 5, 262, 74, 1718, 3],\n [1719, 5, 1, 263, 2, 262, 250, 27, 316, 1716, 1, 893],\n [15, 96, 677, 2, 1, 1055, 3, 1056, 521, 678],\n [10, 127, 35, 101, 283, 10, 231, 83, 3, 97, 10, 1],\n [359, 189, 2, 51, 2521, 3, 1, 213, 2, 51],\n [31, 1057, 3, 596, 5, 25, 14, 36, 41, 495, 190, 2],\n [360, 2, 894, 336, 205, 2, 1307, 217],\n [3, 2, 1, 166, 2, 378, 597, 55, 101, 283, 264, 10],\n [2522, 1, 765, 376, 2, 1, 2523, 326, 3, 496],\n [2, 1, 2524, 1, 2525, 1, 2526, 895, 3, 1, 2527, 2528],\n [74, 2529, 896],\n [],\n [61, 379, 2, 356, 317, 678, 78, 11, 356, 317, 15, 1, 418],\n [2, 300, 141, 63, 598, 102, 1, 2530, 2, 7, 1308, 15],\n [521, 10, 1, 679, 3, 897, 2, 1, 1303, 3, 766, 597],\n [55, 359, 258, 1309, 58, 3, 2531, 15, 58, 61, 190, 2],\n [166, 1310, 5, 19, 680, 15, 1, 418, 2, 190, 63, 1311, 4],\n [2532, 10, 1, 898, 3, 1312, 2, 166, 23, 8, 229, 55],\n [497, 2533, 3, 99, 5, 454, 88, 1274, 1313, 63, 13],\n [1314, 5, 2534, 1720, 7, 95, 114, 2, 1, 1315, 361],\n [63, 13, 597, 55, 317, 5, 173, 3, 2535, 26, 332],\n [25, 14, 6, 1058, 456, 3, 2536, 899, 10, 1, 190, 2],\n [104, 3, 2, 767, 8, 6, 597, 5, 265, 4, 60, 83, 10, 7],\n [231, 380, 900, 3, 7, 231, 901, 1059, 35, 91, 1721],\n [2, 526, 3, 902, 2, 1722, 29, 452, 2537, 414],\n [3, 591, 2538, 664, 15, 96, 677, 26, 9],\n [6, 2539, 4, 2540, 1, 1316, 2, 1, 681, 280, 599, 2, 768],\n [2541, 2542, 3, 2543, 2544, 49, 27, 1723, 312, 21],\n [2545, 3, 21, 2546, 4, 337, 58, 248, 9, 6, 769, 73],\n [16, 29, 662, 3, 56, 32, 903, 769, 224, 5, 67],\n [2547, 27, 71, 653, 69, 2548, 1724, 5, 2549],\n [5, 2550, 2, 31, 767, 1, 278, 521, 10, 232, 127],\n [1, 2551, 898, 5, 530, 1725, 2, 372, 3, 1317],\n [31, 1060, 1318, 1726, 2, 770, 4, 2552],\n [1717, 264, 10, 1318, 600, 108, 43],\n [1727, 771, 81, 4, 1, 582, 1718, 18, 2, 592, 199, 36, 6],\n [7, 231, 121, 109, 123, 27, 1263, 43, 1728, 3, 199, 904],\n [48, 1, 19, 84, 1319, 1, 64, 128, 2, 2553],\n [16, 1729, 276, 1730, 3, 16, 1061, 1, 18, 5],\n [150, 4, 1, 162, 682, 2, 23],\n [],\n [70, 27, 665, 4, 1, 19, 27, 183, 8, 40, 2, 1, 93, 2554],\n [1320, 35, 91, 6, 683, 1, 419, 2, 33, 420, 491],\n [190, 1062, 3, 1, 414, 2555, 3, 381],\n [],\n [14, 37, 17, 59, 215, 2556, 498, 116, 10, 7, 163, 284, 8, 1],\n [19, 71, 151, 527, 8, 905, 2, 190, 2557, 233, 1731, 2],\n [5, 1, 156, 26, 234, 21, 318, 21, 1321, 3, 684, 8],\n [72, 22, 685, 7, 531, 338, 2, 756, 3, 490, 58, 84, 16],\n [41, 17, 4, 13, 678, 5, 54, 64, 112, 284, 2, 30, 231, 1732, 6],\n [686, 33, 2558, 8, 1, 22, 6, 261, 215, 5, 7, 906, 2559, 1733],\n [3, 1734, 112, 1, 532, 1735, 4, 1322, 9, 20, 1736],\n [1063, 12, 687, 102, 1064, 2560],\n [],\n [49, 27, 74, 4, 2561, 1737, 102, 118, 126, 20, 93, 2562, 2563],\n [4, 1, 419, 2, 14, 22, 48, 69, 19, 188, 27, 125, 57],\n [1065, 1066, 183, 1, 758, 532, 4, 13, 8, 72, 22, 2564, 1],\n [163, 533, 99, 3, 104, 2, 1, 42, 9, 1738, 58],\n [1739, 3, 285, 174, 2, 1323, 3, 1740, 9, 362, 58, 44],\n [1741, 44, 534, 3, 286, 44, 2565, 4, 13, 772, 35, 156, 9],\n [2566, 58, 4, 68, 1067, 15, 1, 191, 1324, 2, 382, 1325],\n [1326, 4, 13, 44, 1742, 887, 1743, 49, 17, 1053, 1327, 3],\n [1328, 1, 1068, 74, 69, 96, 188, 56, 1069],\n [1744, 45, 48, 1329, 256, 15, 51, 374, 2567, 57, 60, 17],\n [2568, 1, 1036, 2, 43, 532, 2, 118, 577, 21, 1],\n [662, 266, 2, 1, 42, 57, 123, 451, 29, 84, 4, 175, 5],\n [516, 457, 773, 70, 383, 535, 2, 1, 499, 2, 1, 19],\n [4, 1, 18, 26, 1, 138, 2, 408, 6, 46, 1, 147, 2569],\n [657, 27, 71, 585, 2, 22, 5, 774, 3, 1302, 2, 496, 601],\n [3, 381, 11, 251, 2, 258, 3, 157, 17, 11, 1330, 176],\n [],\n [27, 71, 585, 2, 58, 5, 51, 31, 458, 11, 775, 2, 1],\n [189, 21, 16, 159, 1738, 89, 331, 11, 421, 15, 536],\n [156, 4, 1, 18, 68, 2, 1, 2570, 1057, 2, 409, 23, 3],\n [11, 459, 5, 16, 43, 200, 29, 59, 907, 21, 1, 448, 500, 3],\n [894, 2, 267, 5, 1045, 11, 1331, 55, 16, 1, 19],\n [89, 123, 13, 62, 7, 1070, 161, 2, 285, 409, 23, 174, 2],\n [7, 98, 287, 776, 5, 16, 4, 537, 317],\n [],\n [7, 159, 6, 7, 379, 2, 192, 1071, 339, 132, 38, 20, 384],\n [538, 385, 314, 5, 7, 385, 288, 3, 10, 363, 4, 385],\n [422, 1, 385, 200, 3, 422, 539, 7, 448, 1745, 2, 217],\n [3, 448, 301, 2, 777, 778, 1, 769, 602, 8, 1],\n [91, 19, 316, 1072, 89, 11, 7, 133, 31, 2571, 6],\n [132, 184, 14, 779, 2, 385, 3, 1746, 152, 6, 2572],\n [46, 1, 1747, 5, 1332, 3, 2573, 31, 386, 685, 98],\n [2574, 3, 1748, 36, 6, 84, 4, 60, 68, 152, 4],\n [13, 281, 34, 1333, 133, 2575, 2, 1722, 1073, 2, 1749],\n [3, 1750, 1074, 406, 655, 3, 1751, 5, 1, 688, 1],\n [380, 3, 1, 2576, 2, 31, 386, 20, 1075, 1752, 46],\n [1, 780, 128, 1, 1753, 1754, 2, 1, 91, 19, 6, 8, 9],\n [1755, 4, 1076, 374, 876, 2, 1, 31, 131, 5, 7, 263, 5],\n [16, 1, 73, 2, 1, 31, 288, 20, 1756, 1752],\n [],\n [1, 460, 8, 1043, 70, 88, 20, 62, 1, 2577],\n [670, 2, 19, 23, 6, 17, 484, 4, 2578, 5, 689, 9, 6, 7],\n [460, 5, 380, 2, 288, 3, 908, 11, 40, 1334, 7, 1757],\n [781, 5, 16, 7, 1077, 2, 42, 20, 909, 578, 5, 1],\n [603, 2, 540, 1, 387, 460, 1, 224, 24, 44],\n [12, 293, 1323, 3, 2579, 2580, 3, 2581, 4, 40, 2, 2582],\n [2583, 782, 6, 33, 604, 11, 1335, 4, 2584, 40, 5, 1, 884],\n [451, 4, 118, 865, 910, 2, 1, 19, 6, 2585, 287, 1, 224],\n [6, 1268, 4, 130, 7, 2586, 26, 1, 224, 5, 1, 31, 266, 6],\n [903, 783, 1, 153, 1758, 2, 177, 3, 672, 6, 33, 1336],\n [172, 32, 663, 8, 9, 1078, 268, 486, 4, 1079, 48],\n [2587, 36, 6, 61, 604, 31, 380, 15, 1, 2588, 2],\n [153, 157, 36, 6, 61, 517, 31, 911, 5, 1337, 2589, 451],\n [438, 1, 56, 690, 15, 1337, 6, 7, 2590, 40, 5, 1, 1759],\n [166, 2, 8, 2591, 912, 2, 146, 5, 1, 691, 12, 5, 1],\n [1338, 4, 193, 16, 18, 37, 1339, 5, 299, 1059, 2, 236],\n [5, 2592, 78, 5, 2593, 1, 1080, 2, 141, 33, 605],\n [6, 14, 1, 1760, 908, 8, 15, 40, 18, 4, 461, 158, 5],\n [28, 1081, 37, 302, 7, 19, 2594, 103, 1, 19, 22, 2595, 5],\n [113, 157, 317, 1074, 1761, 174, 2, 201, 1, 93],\n [133, 161, 2, 406, 655, 3, 1340, 239, 7, 2596],\n [488, 4, 2597, 913, 2598, 2, 28, 388, 1324, 103, 285, 22],\n [6, 331, 34, 25, 14, 6, 662, 2599, 236, 174, 2, 201, 7, 161],\n [2, 2600, 16, 2601, 1, 2602, 6, 113, 32, 2603, 5, 914],\n [218, 1, 361, 3, 1762, 1, 364, 2, 1, 40, 2604, 7, 288, 2],\n [218, 883, 2, 1745, 2, 149, 501, 146, 240],\n [1341, 3, 1763, 2, 915, 423, 239, 1, 1764],\n [868, 2, 1, 691, 33, 219, 11, 1751, 1334, 5, 9, 6, 5, 1],\n [912, 2, 1282, 17, 10, 916, 4, 1, 917, 2, 141],\n [2605, 1342, 26, 10, 363, 4, 1, 2606, 2, 22, 2607],\n [1070, 409, 2608, 2, 303, 5, 32, 2609, 26, 25, 1, 44],\n [2610, 112, 1, 19, 23, 1765, 89, 34, 7, 31, 252],\n [],\n [334, 14, 386, 6, 185, 1, 269, 2, 19, 114, 12],\n [131, 2, 194, 131, 6, 113, 7, 257, 16, 6, 1343, 4, 32, 206],\n [49, 50, 29, 1, 206, 5, 408, 2, 1766, 12, 1767, 42, 157, 95],\n [287, 317, 4, 13, 2611, 4, 7, 225, 410, 114, 71, 13, 1082],\n [4, 918, 8, 195, 26, 49, 1, 206, 5, 408, 6, 1, 182, 2, 7],\n [288, 2, 31, 406, 655, 3, 409, 23, 114, 71, 1083],\n [45, 2, 3, 13, 1343, 4, 72, 32, 211, 36, 6, 137, 2, 40, 304, 2],\n [131, 103, 83, 20, 5, 377, 2, 541, 36, 6, 7, 95],\n [2612, 5, 54, 1757, 1084, 36, 6, 17, 2613, 748, 20, 17],\n [578, 5, 1768, 95, 784, 162, 2614, 51, 2615, 20],\n [17, 2616, 38, 20, 17, 919, 51, 424, 139, 3, 33, 38, 20, 425],\n [7, 319, 2, 83, 3, 36, 6, 1, 2617, 1, 2618, 8, 146],\n [24, 152, 26, 45, 2, 1, 230, 45, 2, 425, 83, 8, 20],\n [4, 1054, 146, 3, 45, 2, 425, 43, 5, 7, 31, 3, 406, 896],\n [112, 36, 6, 2619, 7, 114, 2, 30, 66, 595, 3, 542, 69, 147],\n [785, 2, 19, 114, 480, 70, 27, 134, 14, 138, 2, 408],\n [5, 1769, 2620, 27, 25, 658, 8, 1, 56, 114, 8, 920],\n [21, 227, 1, 56, 190, 8, 239, 2621, 6, 8, 678, 55, 23],\n [89, 8, 27, 537, 24, 117, 3, 24, 424, 12, 1, 2622, 2],\n [236, 56, 11, 38, 20, 921, 4, 117, 20, 17, 153, 1770],\n [26, 1, 19, 37, 59, 33, 287, 776, 33, 411, 24, 1, 692],\n [73, 3, 693, 2, 23, 8, 1, 98, 103, 42, 20, 1771],\n [15, 114, 6, 1, 40, 98, 5, 1, 110, 103, 9, 6, 93, 786],\n [4, 134, 2623, 787, 2, 25, 114, 1036, 1, 668, 9, 6, 56],\n [70, 7, 657, 3, 784, 910, 2, 462, 19, 114, 1772],\n [8, 40, 6, 5, 54, 1344, 2, 2624, 8, 2625, 3, 1297, 1085],\n [114, 8, 229, 24, 265, 7, 145, 4, 60, 5, 336, 22],\n [5, 1345, 4, 7, 195, 16, 31, 5, 288, 6, 2626, 1, 293],\n [604, 3, 1086, 5, 2627, 286, 5, 7, 161, 10, 363, 4, 16],\n [891, 53, 13, 2628, 3, 1773, 767, 1346],\n [],\n [1, 111, 257, 4, 463, 5, 86, 82, 669, 1, 419, 48],\n [1, 19, 2, 167, 259, 2, 285, 230, 6, 8, 55, 58],\n [1, 494, 288, 2, 1, 19, 6, 2629, 9, 37, 7, 1283, 4, 2630],\n [89, 10, 23, 4, 302, 1, 85, 2631, 103, 39, 694, 55],\n [426, 258, 174, 2, 201, 56, 7, 98, 4, 537, 317, 265],\n [32, 502, 3, 456, 363, 4, 68, 171, 258, 4, 13, 215, 5],\n [1, 374, 9, 606, 7, 1283, 4, 13, 7, 1087, 409, 32, 1774],\n [159, 14, 6, 1, 543, 320, 3, 24, 14, 788, 695, 3],\n [2632, 2633, 2, 305, 187, 1, 333, 1347, 1775],\n [1, 18, 332, 25, 1776, 5, 1, 22, 17, 15, 1, 418, 2, 1],\n [1777, 26, 15, 1, 418, 2, 1, 1038, 1, 96, 146, 1088],\n [74, 231, 199, 1089, 3, 922, 26, 5, 1, 19, 1, 279],\n [88, 675, 20, 2634, 24, 25, 923, 924, 1, 211, 6, 17],\n [1, 923, 303, 2, 1, 925, 26, 1, 182, 2, 31, 212],\n [3, 500, 9, 6, 14, 2635, 24, 657, 1348, 14, 2636],\n [4, 1, 1035, 2, 1, 232, 288, 8, 362, 43, 191],\n [108, 5, 1, 19, 2637, 2, 196, 3, 670, 2, 173, 3, 100],\n [],\n [1, 301, 2, 25, 1, 1349, 6, 185, 5, 294, 1, 458],\n [2, 294, 6, 8, 9, 1090, 1, 313, 11, 1, 1350, 156, 2, 1],\n [88, 2, 267, 1, 110, 155, 30, 499, 4, 232, 152],\n [6, 293, 79, 7, 110, 232, 526, 3, 1778, 776, 24, 51],\n [1351, 5, 1, 313, 20, 17, 102, 7, 2638, 298, 7, 668, 1, 313],\n [6, 1, 1352, 696, 2, 25, 1779, 540, 9, 6, 28, 495, 1780, 3],\n [2639, 1, 763, 107, 2, 25, 28, 108, 3, 1, 156, 4],\n [865, 2640, 3, 2641, 25, 28, 1778, 1781, 9, 6, 1],\n [111, 1091, 1, 111, 1782, 1, 111, 696, 2, 1, 1353, 2, 1354],\n [355, 3, 1783, 1, 111, 1784, 2, 1785, 1355, 1356],\n [3, 1786, 2, 16, 25, 69, 1787, 3, 2642, 3, 2643, 25],\n [69, 1691, 3, 607, 421, 20, 26, 1, 1092],\n [789, 3, 415, 9, 6, 55, 88, 2644, 21, 14],\n [365, 8, 1093, 37, 62, 30, 340, 3, 885, 325],\n [9, 6, 55, 43, 88, 8, 1, 124, 3, 697],\n [698, 2, 127, 37, 59, 699, 9, 6, 55, 47, 27, 60, 5],\n [3, 10, 1, 110, 8, 27, 522, 30, 289, 3, 690, 30, 303],\n [],\n [5, 96, 168, 14, 140, 8, 43, 88, 5, 1, 19],\n [123, 17, 13, 153, 191, 661, 12, 449, 2, 1357, 2645, 1],\n [1094, 2, 523, 306, 608, 11, 1327, 1328, 12, 1788],\n [26, 285, 670, 2, 341, 500, 48, 133, 97, 3],\n [189, 544, 2, 1316, 1358, 42, 123, 13, 609, 45, 48, 7],\n [790, 2, 1, 503, 182, 2, 267, 1, 359, 458],\n [2, 14, 63, 13, 545, 523, 55, 40, 244, 233, 24, 359],\n [19, 22, 79, 21, 228, 2646],\n [],\n [36, 6, 524, 16, 2647, 44, 2648, 46, 1, 791, 504],\n [1789, 79, 4, 193, 1068, 11, 309, 11, 1326, 2, 2649, 610, 3, 1790],\n [154, 2, 505, 578, 5, 601, 3, 496, 49, 27, 583, 35, 14, 24, 1],\n [329, 2, 603, 2, 1, 1068, 15, 601, 34, 2650, 3, 214],\n [2651, 27, 134, 7, 657, 3, 1095, 2652, 252, 8, 298],\n [1791, 300, 1792, 4, 14, 304, 2, 22, 5, 1, 19, 26, 49],\n [27, 583, 35, 9, 24, 158, 128, 27, 183, 8, 14, 22, 490, 1, 138],\n [2, 1316, 24, 16, 1, 18, 63, 2653, 3, 700, 1, 325, 2],\n [1093, 5, 100, 299, 32, 500, 164, 48, 1, 97, 342, 3],\n [1, 701, 235, 282, 5, 142, 10, 43, 88],\n [1, 503, 182, 2, 267, 6, 2654, 15, 611, 1],\n [42, 20, 101, 237, 1, 763, 1793, 1794, 1, 464, 1096, 1],\n [493, 11, 9, 229, 24, 1, 248, 2, 1, 1049, 49, 27, 241, 238, 58, 4, 1],\n [98, 103, 1, 1049, 20, 2655, 33, 81, 1, 523, 82, 7, 115, 6],\n [62, 2, 43, 97, 24, 1, 329, 2, 51, 770, 4, 1],\n [596, 4, 16, 38, 53, 13, 311, 15, 546, 7, 912, 2, 1, 464],\n [792, 10, 493, 792, 6, 62, 57, 506, 17, 427, 483, 1, 42, 545],\n [612, 8, 1, 602, 15, 1, 1795, 182, 2, 1, 464, 526, 11],\n [899, 10, 1, 1796, 6, 8, 1, 464, 792, 6, 33, 268, 786],\n [4, 218, 21, 283, 24, 1, 1097, 1, 42, 5, 40, 1077, 613, 2656],\n [1797, 2657, 464, 702, 24, 1, 2658, 3, 1097, 3, 1339],\n [5, 299, 45, 293, 79, 40, 2659, 38, 241, 1098, 328, 8, 40],\n [793, 241, 2660, 56, 40, 2661, 7, 389, 21, 283, 3, 241, 547, 703],\n [51, 2662, 2663, 1796, 174, 2, 464, 674, 1309, 64],\n [83, 1099, 11, 2664, 51, 1343, 1348, 41, 1, 2665],\n [2, 1, 464, 792, 11, 899, 10, 8, 2, 493, 1, 1284, 2666],\n [175, 40, 704, 2, 32, 2667, 5, 761, 143, 1, 581, 1798, 4, 440],\n [2668, 5, 761, 164, 8, 1, 702, 2, 464, 20, 2669, 3, 60, 17],\n [2670, 339, 143, 1, 493, 37, 7, 95, 2671, 16, 362, 1],\n [702, 1799, 139, 2672, 1, 326, 1, 42, 613, 14],\n [45, 15, 428, 10, 1, 359, 107, 2673, 21, 321, 3],\n [501, 24, 1, 225],\n [],\n [38, 82, 675, 1, 189, 170, 15, 384, 1, 702, 78, 48],\n [2674, 38, 2675, 1, 101, 2676, 15, 1300, 1, 1800, 2677, 2],\n [2678, 10, 2679, 2680, 5, 58, 15, 2681, 9, 45, 38, 2682, 1],\n [1359, 377, 15, 326, 1, 1800, 2683, 1360, 12, 68, 64],\n [2684, 55, 16, 1, 493, 6, 1346, 3, 16, 11, 9, 6, 2685],\n [2686, 45, 1, 792, 457, 1, 1100, 16, 41, 2687, 34, 1, 2688, 143],\n [1, 42, 1361, 1, 493, 5, 51, 747, 483, 9, 41, 764, 1101],\n [45, 3, 2689, 46, 9, 82, 1, 42, 20, 680, 4, 1, 1362],\n [457, 5, 503, 131, 384, 9, 45, 1801, 139, 548],\n [30, 249, 3, 2690, 30, 926, 17, 56, 46, 8, 246],\n [526, 26, 46, 449, 2, 31, 2691, 14, 112, 1363, 5, 2692, 1],\n [494, 377, 78, 4, 1, 91, 358, 888, 3, 25, 8, 549, 10],\n [1, 588, 2, 173, 5, 1, 77, 2, 69, 91, 575, 361, 57],\n [203, 17, 705, 2, 1, 173, 282, 5, 1802, 115, 2, 1, 702],\n [2, 927, 757, 1, 73, 187, 16, 763, 97, 20],\n [1364, 1, 111, 670, 2, 1102, 3, 528, 1, 1365],\n [282, 5, 1, 2693, 2, 492, 928, 250, 2, 1, 340],\n [2694, 751, 16, 43, 877, 29, 120, 46, 1103, 50],\n [63, 2695, 1, 100, 2, 25, 1093, 48, 1, 489, 2, 1],\n [1794, 464, 3, 493, 702, 48, 674, 57, 60, 17, 771, 8, 14, 6],\n [1, 56, 12, 1, 310, 453, 26, 9, 6, 353, 8, 95, 268, 231, 3],\n [357, 2696, 4, 1, 1104, 2, 1, 100, 2, 1, 794, 20],\n [139, 2697, 1, 86, 6, 680, 4, 81, 44, 543, 3],\n [1105, 1106, 79, 587, 5, 1, 885, 3, 929],\n [1366, 8, 1299, 1079, 15, 100],\n [],\n [151, 47, 6, 353, 2, 14, 40, 546, 2, 702, 342, 5, 1803, 3, 2],\n [194, 57, 29, 56, 1024, 2, 40, 12, 204, 169, 390, 2, 8, 6],\n [353, 5, 30, 690, 2, 315, 107, 342, 5, 315, 230, 3, 2],\n [1, 189, 1804, 1, 230, 930, 1, 18, 10, 7, 1070],\n [380, 9, 490, 87, 117, 35, 101, 283, 9, 931, 87, 48, 264],\n [10, 682, 9, 207, 25, 14, 26, 5, 1367, 9, 6, 2698],\n [706, 21, 2699, 48, 30, 503, 3, 31, 795, 3],\n [341, 2700, 10, 1, 90, 2, 1, 85, 86, 5, 212],\n [3, 213, 9, 1805, 4, 13, 7, 2701, 230, 312, 3, 239],\n [44, 3, 44, 7, 263, 32, 1368, 32, 1806, 2, 2702, 6],\n [1807, 2703],\n [],\n [14, 5, 665, 37, 30, 1369, 46, 1, 932, 2, 173, 187],\n [91, 73, 25, 152, 4, 13, 2704, 37, 4, 13, 426],\n [1808, 3, 1809, 21, 1, 341, 2705, 6, 7, 707, 2, 796],\n [173, 14, 142, 125, 1370, 30, 98, 5, 67, 9, 6],\n [17, 56, 8, 1, 88, 1, 33, 420, 491, 12, 333, 22, 5],\n [1, 19, 130, 1, 405, 15, 1, 419, 2, 173, 16],\n [2706, 58, 16, 362, 58, 107, 2707, 10, 289],\n [174, 2, 201, 153, 661, 2, 283, 3, 797, 26, 8, 1, 341],\n [500, 139, 594, 239, 32, 2708, 1368, 2, 218, 3],\n [285, 1306, 5, 760, 31, 23, 1810, 1808, 2709, 2, 1],\n [2710, 11, 40, 126, 5, 28, 2711, 207, 17, 656, 28, 66, 149, 26, 118],\n [2, 68, 64, 267, 9, 6, 69, 31, 121, 151, 102, 44, 1107, 79, 5],\n [1, 106, 2, 1810, 8, 277, 498, 1811, 123, 2712, 5],\n [1, 284, 2, 1, 40, 126, 207, 1, 22, 8, 28, 152, 123],\n [29, 289, 4, 366],\n [],\n [70, 88, 5, 1, 19, 20, 1371, 5, 14, 2713, 3, 1812],\n [112, 57, 63, 56, 708, 1372, 5, 1813, 35, 1, 1108, 33, 261, 1373],\n [8, 72, 88, 20, 45, 2, 98, 5, 1, 19, 132, 38],\n [20, 1814, 1095, 12, 102, 2714, 5, 51, 550, 9],\n [687, 798, 4, 612, 8, 118, 126, 92, 43, 1108, 71, 429, 5],\n [412, 158, 110, 1, 110, 5, 16, 93, 2, 227, 429, 6, 7, 110, 5],\n [16, 2715, 37, 7, 1374, 3, 230, 84, 4, 60, 68, 20],\n [1743, 3, 236, 20, 2716, 26, 1, 111, 257, 15, 40, 11],\n [15, 1, 64, 6, 8, 216, 123, 29, 120, 1, 67, 16, 1375],\n [87, 4, 193, 334, 28, 497, 22, 25, 36, 6, 5, 9, 2, 247, 3, 232],\n [458, 109, 160, 2, 1, 1804, 20, 592, 153, 2717, 4, 1],\n [1815, 16, 38, 1376, 14, 53, 13, 646, 5, 145, 4, 1, 1377],\n [89, 12, 1, 1347, 16, 1378, 33, 81, 924, 46, 1, 925, 2],\n [1, 1377, 26, 9, 6, 551, 646, 5, 247, 145, 4, 1, 320, 8],\n [1, 2718, 37, 120, 61, 405, 4, 1379, 28, 205, 3, 28],\n [777, 500, 11, 4, 1, 31, 3, 341, 795, 185, 5, 28],\n [22, 35, 91, 1, 552, 16, 1109, 35, 1, 252, 2, 1, 333],\n [188, 20, 614, 452, 1816, 12, 1064, 2719, 933],\n [1, 19, 270, 483, 1, 430, 2, 541, 3, 492],\n [20, 1817, 1110, 338, 2, 5, 1, 154, 2, 1111, 3, 1112],\n [483, 38, 20, 1314, 5, 31, 934, 1380, 21, 340],\n [698, 935, 3, 1818, 21, 341, 251, 27],\n [551, 20, 5, 61, 936, 102, 4, 2720, 1, 696, 2, 69, 923],\n [2721, 81, 293, 4, 431, 10, 58, 1819],\n [],\n [49, 27, 260, 248, 7, 615, 1381, 27, 183, 7, 191, 2722, 2, 157],\n [1, 709, 1113, 2, 157, 6, 451, 7, 1820, 40, 157, 41],\n [7, 1114, 148, 14, 41, 7, 170, 195, 2, 31, 73, 36],\n [74, 17, 5, 1382, 54, 140, 21, 16, 1, 2723, 241, 1115],\n [29, 391, 4, 124, 307, 43, 74, 1821, 78, 3, 1822],\n [573, 5, 2724, 2, 43, 36, 74, 35, 310, 56, 7, 615, 3, 9],\n [937, 351, 3, 1706, 603, 4, 13, 938, 4, 60, 447, 10],\n [58, 7, 553, 2725, 2, 157, 16, 2726, 1, 2727, 2, 710],\n [3, 16, 2728, 9, 45, 4, 1, 1823, 187, 2729, 1250, 41, 1],\n [880, 308, 2, 43, 73, 26, 11, 7, 178, 195, 2],\n [1, 333, 762, 2, 16, 27, 29, 59, 383, 14, 37, 59],\n [662, 1265, 41, 1383, 9, 41, 62, 600, 424, 2730],\n [1824, 74, 2731, 3, 2732, 11, 7, 195, 2, 1, 1678, 3],\n [1287, 2733, 1295, 3, 1692, 2734, 21, 2735, 3],\n [1783, 41, 420, 48, 201, 1825, 37, 59, 1826, 484, 1384],\n [2, 654, 10, 30, 2736, 1116, 2, 149, 1827],\n [1699, 1, 195, 37, 59, 32, 124, 762, 157, 37],\n [59, 311, 48, 2737, 143, 36, 686, 6, 3, 1117, 335, 65],\n [13, 7, 246, 1114, 265, 1, 392, 271, 2, 322, 5, 283],\n [7, 1385, 711, 1114, 6, 2738, 45, 2, 1, 202, 9, 6],\n [32, 2739, 213, 6, 61, 1386, 32, 2740, 2741, 9, 37, 59],\n [2742, 9, 6, 909, 799, 5, 25, 1, 2743, 2, 159, 89],\n [],\n [9, 6, 484, 4, 193, 8, 14, 762, 11, 800, 1, 97, 2],\n [213, 1387, 10, 9, 7, 783, 224, 5, 1, 266, 2, 1],\n [172, 801, 2, 32, 124, 304, 2744, 5, 46, 227, 5, 25, 712],\n [2, 459, 1, 312, 124, 23, 1, 23, 2, 1828, 3, 2],\n [157, 139, 606, 7, 268, 1698, 303, 2745, 3, 2746],\n [174, 2, 201, 2747, 2, 2748, 20, 939, 168, 2, 2749],\n [],\n [26, 25, 14, 140, 7, 170, 224, 5, 1, 266, 2, 1, 19, 40],\n [2, 16, 27, 20, 11, 199, 219, 24, 1388, 1, 163, 1118, 69, 19],\n [251, 3, 4, 7, 268, 616, 1067, 69, 375, 20, 1389],\n [24, 1, 270, 70, 157, 3, 617, 2, 95, 554, 1829],\n [11, 38, 506, 1, 56, 391, 4, 157, 74, 25, 357, 1, 713],\n [2, 14, 270, 20, 686, 1119, 5, 432, 102, 103, 1, 618],\n [251, 3, 176, 29, 59, 662, 27, 687, 940, 1, 419],\n [2, 491, 190, 196, 3, 173, 48, 1, 169, 3, 102, 1],\n [619, 256, 2750, 34, 1, 667, 8, 38, 1830, 683, 1],\n [492, 2, 2751, 38, 2752, 24, 69, 91, 1390, 2],\n [1812, 941, 242, 1, 138, 2, 14, 2753, 105, 13, 2754],\n [49, 9, 74, 17, 261, 33, 676, 11, 4, 92, 9, 1753, 9, 6, 69, 91],\n [67, 16, 6, 802, 1120, 40, 2755, 3, 657, 9, 6],\n [32, 67, 2756, 438, 2757, 21, 1, 1391, 785, 2],\n [157, 9, 6, 84, 16, 942, 15, 1, 93, 145, 113, 4, 1],\n [124, 943, 2, 69, 2758, 69, 443, 4, 537, 4, 2759],\n [141, 3, 4, 134, 432, 2, 1, 554, 2, 157, 17, 4, 69],\n [552, 3, 1320, 4, 92, 4, 60, 4, 1831, 4, 1054, 393],\n [5, 1, 161, 2, 714, 12, 2, 196, 1, 268, 320, 8, 491, 190],\n [196, 3, 173, 20, 1832, 4, 11, 306, 11, 2760, 683, 153],\n [2761, 6, 2, 89, 11, 681, 2762, 11, 241, 13, 1833, 4, 1],\n [1120, 211, 16, 1686, 1121, 67, 507, 67, 120],\n [59, 2763, 1392, 10, 1, 1336, 124, 1122],\n [10, 157, 11, 72, 25, 43, 97, 3, 251, 105, 13, 1834],\n [105, 13, 2764, 10, 1, 2765, 2766],\n [],\n [244, 85, 350, 2, 7, 1261, 3, 648],\n [],\n [143, 190, 15, 1, 2767, 2, 157, 6, 433, 11, 1, 542],\n [2, 242, 12, 7, 941, 67, 1, 190, 2, 7, 2768, 7],\n [2769, 7, 2770, 7, 2771, 7, 2772, 7, 2773, 12, 7, 2774, 1835],\n [6, 433, 11, 555, 306, 3, 803, 1, 195, 6, 8],\n [16, 27, 193, 94, 227, 2775, 902, 48, 2776, 192],\n [3, 2777, 116, 1, 944, 2, 394, 3, 367, 298, 76, 1393, 1394],\n [2, 1, 494, 19, 1695, 2778, 2779, 4, 47, 27, 1123, 508],\n [67, 56, 745, 1393, 1394, 4, 1, 945, 2, 69, 553, 19, 143, 81],\n [44, 79, 906, 804, 34, 12, 255, 1, 2780, 2, 1, 2781, 198, 2, 1],\n [169, 945, 1, 715, 177, 2, 1, 707, 20, 8, 5, 1, 111],\n [1395, 2, 232, 1124, 1, 1385, 124, 99, 6, 17],\n [620, 38, 29, 1, 33, 420, 191, 364, 3, 1396],\n [5, 160, 2, 118, 5, 946, 21, 127, 124, 99, 6, 1836],\n [31, 73, 1125, 30, 947, 790, 1837, 21, 219],\n [1, 278, 379, 2, 756, 804, 19, 11, 1397, 11, 38, 29, 597],\n [1, 2782, 2, 157, 11, 1397, 11, 38, 29, 327, 2, 1, 554],\n [2, 530, 716, 3, 2783, 4, 13, 2, 191, 77, 4, 58, 5],\n [299, 7, 258, 143, 69, 96, 1749, 20, 1398, 2, 242],\n [1, 182, 2, 2784, 119, 11, 1, 206, 3, 211, 2, 67],\n [1, 111, 1395, 2, 118, 126, 1079, 187, 1, 1838, 2, 1, 19],\n [916, 9, 56, 11, 7, 2785, 191, 2786, 10, 16, 4, 134, 2787, 3],\n [2788, 327, 4, 2789, 45, 7, 1839, 23, 49, 27, 74, 4, 585, 69],\n [96, 206, 3, 211, 5, 7, 293, 2790, 112, 49, 27, 74, 4, 904],\n [48, 96, 189, 1, 108, 16, 556, 4, 118, 865],\n [620, 99, 6, 4, 60, 3, 4, 92, 27, 125, 183, 1, 338, 2, 1],\n [19, 46, 30, 876, 4, 13, 44, 531, 44, 2791, 1276, 44],\n [2, 242],\n [],\n [244, 85, 350, 2, 7, 867],\n [],\n [26, 703, 125, 57, 92, 14, 2792, 621, 1, 604, 320, 6],\n [8, 69, 31, 23, 37, 2793, 7, 1840, 3, 769, 224, 49],\n [69, 67, 6, 4, 29, 54, 289, 15, 23, 9, 71, 1079, 55, 32],\n [903, 358, 749, 14, 749, 6, 17, 84],\n [4, 587, 1841, 4, 13, 2794, 5, 7, 389, 21, 717, 498, 9, 6],\n [323, 5, 325, 118, 1842, 2, 69, 19, 188, 16, 261],\n [587, 102, 4, 118, 93, 909, 948, 10, 58, 4, 175, 524],\n [2, 51, 2795, 4, 13, 153, 480, 2, 482, 153, 1667],\n [334, 1, 19, 2796, 20, 5, 378, 1725, 3, 1843, 2],\n [489, 1, 419, 2, 285, 88, 2, 127, 115, 2],\n [169, 173, 2, 196, 2, 100, 1, 2797, 2, 1, 312],\n [622, 3, 673, 4, 7, 619, 936, 1, 224, 5, 1, 589],\n [19, 908, 5, 1, 150, 2, 756, 3, 2798, 114],\n [1, 419, 2, 44, 285, 1399, 3, 445, 1400],\n [2799, 43, 20, 17, 153, 2800, 38, 20, 1057, 2, 1],\n [278, 31, 489, 9, 1401, 26, 4, 1072, 25, 43, 415],\n [4, 805, 58, 5, 51, 1312, 2, 289, 3, 4, 311, 1, 149, 3],\n [713, 282, 48, 358, 2801, 1113, 2, 69, 19],\n [188, 4, 60, 14, 140, 4, 92, 216, 40, 2, 69, 256, 32, 1774],\n [409, 23, 285, 10, 775, 2, 88, 8, 1844, 1, 23],\n [2, 1, 278, 159, 3, 2802, 706, 10, 1, 288, 2, 196],\n [100, 3, 173, 70, 1, 19, 1845, 3, 1846, 216, 18],\n [2, 159, 48, 2803, 334, 72, 7, 137, 409, 1847],\n [87, 10, 1, 288, 2, 1402, 3, 949, 87, 10, 1, 806, 2],\n [676, 445, 446, 27, 123, 29, 1, 1700, 3, 310, 1403, 2],\n [7, 278, 159, 16, 6, 950, 2804, 3, 2805],\n [],\n [244, 85, 350, 2, 747, 326],\n [],\n [244, 85, 350, 2, 7, 1640, 326],\n [],\n [],\n [],\n [],\n [1, 19, 3, 1, 23, 2, 1, 18],\n [],\n [],\n [],\n [],\n [1022],\n [],\n [1, 19, 3, 1, 23, 2, 1, 18],\n [],\n [],\n [951, 952, 57, 1404, 4, 311, 255, 50, 1, 499, 80, 1, 19],\n [3, 1, 278, 23, 2, 1, 409, 3, 1, 249, 15, 95],\n [480, 5, 1, 251, 3, 97, 2, 19, 22, 8, 9, 557, 13],\n [523, 1405, 4, 91, 31, 200],\n [],\n [592, 57, 434, 4, 583, 35, 1, 148, 24, 1, 64, 128, 3, 878, 1],\n [499, 2, 1, 19, 4, 1, 23, 3, 182, 2, 1, 42, 5],\n [1, 19, 11, 9, 6, 786, 4, 586, 228, 235, 10, 72],\n [605, 807, 83, 11, 137, 42, 57, 29, 233, 1, 2806],\n [2, 1848, 7, 111, 431, 2, 2807, 148, 24, 1, 22, 2, 1],\n [144, 169, 19, 8, 5, 68, 690, 50, 53, 805, 1],\n [112, 5, 16, 1, 149, 485, 22, 428, 45, 5, 359, 367],\n [],\n [68, 615, 154, 519, 57, 41, 1849, 94, 1, 19, 527, 2808, 5, 1],\n [582, 1050, 4, 183, 1126, 3, 1850, 16, 1406, 605, 2809],\n [24, 25, 544, 2, 2810, 2811, 3, 2812, 1, 200],\n [2, 1, 42, 27, 120, 7, 111, 431, 2, 558, 5, 684, 47, 27],\n [417, 3, 718, 40, 2813, 44, 504, 79, 1, 1127, 62],\n [14, 1128, 1407, 873, 2814, 27, 29, 17, 47, 50, 487, 50, 487, 84],\n [35, 16, 1, 42, 53, 22, 43, 20, 25, 15, 1129, 116, 8, 2815],\n [1, 465, 2, 1, 462, 67, 184, 11, 1, 2816, 63, 238, 7],\n [2817, 12, 204, 3, 1851, 1, 147, 889, 33, 49, 27, 311, 255, 1],\n [2818, 797, 1, 692, 688, 10, 30, 2819, 2, 2820, 1126, 1852],\n [5, 2821, 131, 1853, 339, 33, 8, 36, 123, 13, 11, 137],\n [799, 509, 11, 171, 1126, 438, 25, 2, 1, 129, 1130, 10, 184],\n [1854, 327, 4, 338, 424, 2822, 3, 886, 3, 1408, 7, 1855, 68],\n [1850, 1, 808, 2823, 3, 1115, 7, 615, 1131, 27, 63, 1851],\n [1, 56, 96, 152, 8, 63, 1115, 260, 34, 5, 72, 7, 98, 9],\n [6, 25, 62, 2824, 2825, 113, 809, 317, 45, 2, 7, 439],\n [6, 56, 158, 595, 2, 1129, 9, 1409, 1, 1856, 2, 40, 86],\n [46, 158, 1, 266, 2, 1129, 140, 1132, 383],\n [1857, 2826, 8, 36, 20, 95, 297, 62, 97, 16],\n [20, 36, 16, 29, 59, 1133, 21, 1, 19, 1134, 1],\n [1858, 1, 225, 3, 2, 16, 1, 18, 6, 4, 238, 5, 11, 81, 11],\n [171, 5, 1, 664, 171, 106],\n [],\n [36, 6, 268, 137, 98, 5, 1, 462, 688, 15, 1, 18],\n [4, 22, 1, 1084, 1, 466, 1, 97, 1, 395, 10],\n [16, 1, 18, 53, 2827, 1831, 3, 909, 2828, 3, 102],\n [1, 1410, 1854, 29, 59, 15, 1, 93, 145, 2829, 1, 83, 8],\n [29, 4, 60, 10, 43, 189, 29, 17, 102, 7, 1135, 719],\n [98, 5, 67, 38, 20, 47, 1, 96, 1859, 126, 1032],\n [2830, 5, 1, 497, 1824, 1066, 709, 2831, 3, 2832, 116, 7, 2833],\n [545, 612, 2834, 8, 368, 120, 59, 1860, 179, 256, 1050, 4],\n [183, 40, 103, 152, 34, 1, 145, 2, 1, 42, 2835, 1, 300],\n [2, 141, 34, 1, 145, 2, 1, 225, 12, 103, 1, 42, 120, 68],\n [380, 15, 2836, 1, 141, 368, 2837, 368, 330, 1861, 343],\n [179, 256, 255, 368, 185, 810, 101, 546, 57, 53, 1408, 8],\n [8, 41, 17, 5, 14, 582],\n [],\n [158, 257, 8, 6, 1136, 21, 43, 2838, 10, 51, 287],\n [1126, 6, 8, 953, 6, 1411, 15, 2839, 11, 247, 1862],\n [2, 42, 11, 171, 15, 450, 10, 42, 2840, 2841, 11, 32],\n [2842, 2, 2843, 2844, 250, 8, 38, 13, 1863, 1864, 1],\n [1865, 42, 1137, 38, 2845, 428, 38, 1290, 4, 13, 7],\n [1412, 3, 302, 1, 1866, 954, 1124, 8, 27, 20, 1138],\n [10, 45, 2, 19, 5, 1, 156, 1, 1413, 34, 1, 1747, 3, 5],\n [1, 591],\n [],\n [34, 1, 129, 252, 6, 2846, 1, 1414, 2, 277, 3, 375],\n [49, 953, 6, 34, 7, 2847, 252, 50, 63, 29, 1414, 2],\n [107, 3, 277, 1, 955, 3, 1, 439, 16, 2848, 1, 955],\n [2849, 1, 263, 16, 6, 1075, 15, 25, 36, 6, 457, 4, 61],\n [405, 15, 2850, 4, 1867, 956, 3, 811, 36, 6, 7],\n [95, 2851, 784, 2852, 297, 62, 146, 3, 1868],\n [4, 13, 597, 21, 25, 42, 1075, 5, 7, 237, 106, 9, 6, 5, 2853],\n [4, 14, 539, 8, 1, 375, 37, 59, 699, 24, 1, 169],\n [19, 78, 55, 1, 467, 36, 6, 184, 33, 81, 720, 213],\n [3, 36, 20, 184, 33, 160, 417, 306, 1868, 5, 1],\n [110, 82, 229, 1, 1869, 121, 2, 2854, 14, 21, 1, 957],\n [610, 12, 2855, 154, 2, 19, 23, 151, 130, 1, 42, 315],\n [198, 184, 1, 2856, 2857, 2, 1, 2858, 3, 21, 1, 106, 38],\n [29, 1711, 38, 65, 29, 1415, 1, 147, 21, 1870, 33, 81],\n [667, 933, 14, 1871, 12, 389, 12, 952, 12, 198, 953, 229, 45, 10],\n [1872, 2859, 35, 1, 2860, 1, 42, 29, 17, 1139],\n [47, 38, 29, 958, 711, 1, 901, 2, 25, 14, 6, 2861],\n [2862, 1643, 2, 1, 481, 2863, 62, 4, 87, 21, 32, 96],\n [2864, 5, 2865, 8, 33, 160, 2866, 2, 42, 74, 809, 35],\n [7, 237, 1871, 175, 1140, 2867, 184, 72, 7, 959, 5, 294, 3, 5],\n [40, 2, 69, 66, 2868, 1697, 14, 2869, 2870, 342, 4, 13, 2871, 4],\n [1873, 1874, 21, 30, 1134],\n [],\n [57, 53, 29, 2872, 1058, 5, 131, 4, 92, 1786, 1, 279, 544],\n [2, 1, 280, 67, 30, 1857, 2, 266, 30, 701, 2873],\n [2, 42, 30, 1414, 2, 375, 3, 277, 9, 53, 13, 1875],\n [78, 21, 1876, 8, 1, 453, 2, 1877, 6, 396, 1, 18, 9, 6, 5],\n [1, 225, 1, 1416, 1249, 3, 1272, 50, 1141, 510, 5],\n [1, 529, 430, 3, 108, 2, 1, 18, 366, 34, 8],\n [252, 36, 6, 17, 81, 4, 13, 330, 94, 1, 23, 2, 1, 18, 7, 681],\n [431, 557, 13, 330, 94, 1, 809, 2, 1, 18, 26, 1, 19, 6],\n [17, 1, 98, 103, 1, 18, 397, 151, 1, 224, 16, 6, 580],\n [48, 69, 67, 6, 1, 1878, 2, 1, 453, 2, 1877, 9, 6, 7],\n [224, 7, 762, 17, 2874, 8, 680, 21, 2875, 70, 1],\n [2876, 453, 2877, 24, 1, 313, 4, 1, 1417, 5, 14, 707, 1],\n [18, 239, 1, 1417, 94, 16, 1, 1418, 2, 67, 2878, 39],\n [6, 1, 453, 94, 16, 38, 20, 721],\n [],\n [49, 27, 238, 32, 611, 24, 32, 352, 156, 103, 1, 1278, 6, 504],\n [327, 4, 653, 47, 6, 310, 15, 1, 18, 3, 6, 938, 4, 527],\n [47, 6, 417, 27, 183, 1, 18, 157, 55, 1, 31, 2879],\n [3, 2880, 2, 1, 1413, 36, 20, 95, 544, 2, 99],\n [3, 303, 4, 87, 5, 1, 559, 281, 34, 1142, 20, 62],\n [1879, 788, 1419, 20, 1143, 3, 1, 18, 722, 694],\n [39, 186, 28, 423, 28, 1880, 20, 1881, 250, 1],\n [18, 2881, 5, 1, 414, 88, 3, 1807, 606, 752],\n [2, 526, 131, 3, 916, 15, 1, 1259, 3, 149, 2, 236, 3],\n [1, 543, 812, 2, 2882, 28, 108, 4, 1, 228],\n [99, 2, 1, 414, 1306, 5, 43, 414, 1720, 239],\n [32, 405, 15, 1094, 213, 1, 352, 156, 105, 486, 29],\n [7, 1084, 103, 1, 18, 241, 22, 45, 28, 336, 430],\n [9, 105, 29, 7, 1087, 466, 5, 16, 28, 1879, 241, 13],\n [426, 1, 23, 2, 1, 18, 105, 2883, 45, 2, 1882, 4, 1, 766],\n [1420, 1144, 3, 2884, 39, 105, 29, 28, 1883, 28, 1884],\n [3, 1885, 5, 16, 1, 278, 110, 45, 2, 1882, 105, 593, 4, 87],\n [],\n [151, 49, 27, 1072, 3, 1886, 25, 2, 14, 27, 29, 1, 352, 19],\n [36, 6, 61, 2885, 94, 9, 61, 2886, 1421, 2, 1887, 12],\n [96, 394, 9, 6, 113, 7, 202, 2, 425, 1817, 3],\n [5, 7, 247, 504, 3, 2887, 112, 47, 15, 167, 532, 63],\n [13, 215, 5, 93, 2888, 56, 5, 7, 1132, 2889, 3, 1888],\n [1145, 5, 1, 101, 98, 1, 352, 156, 37, 4, 13, 1034, 1, 18],\n [71, 13, 373, 48, 264, 10, 44, 1364, 192, 3, 10, 44],\n [42, 5, 131, 8, 36, 53, 13, 1, 2890, 3, 2891, 31, 23],\n [723, 1, 88, 3, 2892, 2, 1, 156, 365, 20],\n [17, 2893, 560, 15, 1, 90, 2, 1, 18, 1, 758, 356, 6],\n [84, 813, 3, 47, 1, 18, 63, 134, 45, 2, 58, 6, 1089],\n [286, 1, 203, 2, 7, 19, 5, 14, 19, 1, 23, 2, 1, 18, 239],\n [1, 25, 1105, 211, 25, 1, 2894, 170, 4, 1146, 1, 90, 2],\n [1, 18, 453, 36, 157, 551, 26, 258, 960, 3],\n [157, 55, 3, 5, 150, 4, 14, 258, 70, 27, 238, 1, 23, 2],\n [1, 18, 1422, 3, 721, 5, 14, 112, 27, 60, 17, 183, 8, 39, 6],\n [101, 2, 25, 7, 1129, 201, 412, 1, 1889],\n [],\n [1, 481, 33, 2895, 62, 8, 67, 140, 2896, 2897, 6],\n [1423, 49, 27, 771, 113, 4, 961, 9, 10, 1, 377, 2, 2898],\n [5, 26, 332, 25, 9, 6, 786, 4, 586, 1, 165, 2, 350, 45],\n [10, 1, 692, 1147, 2, 1, 18, 2, 440, 343, 962, 12, 724],\n [154, 2, 505, 39, 6, 323, 1890, 295, 2899, 295, 10, 108],\n [2, 25, 712, 39, 6, 17, 7, 555, 2900, 201, 946, 1, 468, 37, 4],\n [1148, 10, 111, 2901, 3, 608, 5, 131, 764, 4, 1424, 45, 68],\n [1822, 2902, 2, 152, 1, 18, 6, 323, 1866, 285, 3, 1],\n [202, 2, 67, 6, 1, 202, 2, 814, 338, 2, 28, 108],\n [2, 300, 58, 446, 55, 446, 55, 721, 77, 38],\n [1830, 683, 815, 146, 174, 2, 1891, 12, 201, 1149, 4],\n [312, 1892, 308],\n [],\n [49, 27, 463, 14, 255, 227, 1, 558, 57, 183, 2903, 5, 1, 755],\n [2, 160, 192, 669, 47, 6, 879, 1, 136, 67, 6, 17, 33, 81],\n [1425, 11, 1893, 9, 1426, 7, 202, 261, 816, 6, 49, 50],\n [1311, 10, 1, 85, 149, 552, 3, 226, 25, 33, 1427, 33],\n [1428, 3, 1891, 33, 137, 1894, 12, 2904, 109, 6, 39],\n [331, 4, 134, 1, 170, 114, 242, 3, 141, 49, 36],\n [74, 61, 112, 593, 4, 227, 510, 4, 1895, 3, 1896, 43, 552, 2],\n [1, 18, 1, 202, 557, 309, 13, 816, 27, 125, 614, 29, 4],\n [1429, 3, 2905, 1, 108, 12, 813, 4, 1897, 58, 26, 49, 27, 29],\n [386, 2, 725, 3, 2, 97, 36, 6, 158, 2906, 593],\n [4, 227, 27, 63, 178, 1, 85, 108, 300, 58, 1150, 538],\n [95, 314, 3, 63, 139, 1151, 78, 4, 1, 2907, 16, 1898, 920, 35],\n [1, 206, 2, 1, 1899, 675],\n [],\n [2908, 1430, 74, 2909, 2910, 105, 2911, 116, 296, 38, 20, 17, 296],\n [416, 4, 1900, 32, 364, 12, 99, 140, 4, 22, 9, 45, 3],\n [384, 9, 45, 623, 1890, 78, 817, 1152, 939, 1138],\n [10, 97, 1901, 894, 1153, 1902, 898],\n [9, 2, 249, 623, 2912, 2, 2913, 930],\n [213, 238, 1, 611, 2, 1, 137, 18, 126, 1280, 4, 92, 7, 1903],\n [49, 39, 2914, 1045, 10, 1, 205, 12, 434, 39, 551, 65, 17, 134],\n [114, 26, 70, 39, 1904, 4, 658, 28, 364, 9, 6, 7, 202],\n [2, 214, 28, 165, 344, 214, 9, 48, 7, 963, 2, 814, 1, 624],\n [595, 2, 774, 2915, 1, 210, 417, 300, 58, 1, 170],\n [1905, 119, 36, 6, 282, 1, 603, 2, 97, 1],\n [2916, 2917, 1, 2918, 214, 25, 1, 2919, 3, 818],\n [4, 1026, 213, 2, 395, 3, 189, 6, 880, 49, 1, 18],\n [2920, 28, 318, 3, 362, 1, 1903, 36, 6, 964, 2, 405],\n [4, 911, 114, 3, 1906, 4, 1150, 488, 5, 1907],\n [1152, 3, 4, 2921, 11, 309, 7, 111, 431, 2, 141],\n [],\n [33, 1431, 1, 137, 18, 126, 2922, 39, 105, 469, 4, 1154, 37],\n [137, 165, 2, 47, 9, 140, 12, 1432, 12, 47, 9, 1155, 9, 6, 113],\n [7, 443, 4, 2923, 1433, 116, 442, 4, 1908, 1, 108, 2, 819],\n [192, 3, 9, 6, 2924, 171, 4, 881, 1434, 470, 4, 8],\n [2925, 3, 113, 1897, 8, 99, 26, 245, 354, 49, 1, 364],\n [6, 2926, 1044, 9, 1292, 78, 817, 1, 359, 110, 2, 965],\n [73, 4, 16, 9, 71, 2927, 89, 3, 36, 250, 208],\n [5, 1, 415, 2, 114, 3, 213, 40, 2, 1, 42, 892],\n [2928, 1909, 35, 265, 4, 22, 83, 45, 21, 7, 351, 277, 2],\n [820, 3, 330, 2929, 60, 27, 2930, 10, 14, 2931, 700, 7],\n [1910, 5, 7, 1154, 439, 116, 1, 225, 816, 1, 42, 103, 1, 1910],\n [872, 24, 3, 1, 559, 2932, 8, 49, 38, 113, 675, 14],\n [38, 105, 17, 547, 1, 532, 15, 47, 38, 74, 425, 38, 74],\n [82, 412, 1435, 4, 260, 34, 10, 1, 625, 22, 4, 700, 8],\n [22, 65, 451, 130, 32, 244, 2, 184, 1, 138, 5, 202],\n [51, 230, 1911, 8, 389, 4, 13, 1, 381, 2, 1156, 11, 214],\n [7, 966, 24, 1, 381, 2, 967, 4, 8, 2, 2933, 5, 131],\n [4, 134, 7, 252, 2, 912, 38, 101, 2934, 1, 2935],\n [540, 789, 5, 1, 967, 3, 62, 7, 2936, 912, 10],\n [118, 185, 5, 1912, 139, 38, 185, 8, 1, 1913, 792, 12, 2937],\n [5, 967, 2938, 4, 1, 2939, 2940, 5, 1912, 300],\n [1, 779, 2, 161, 3, 1436, 38, 185, 8, 1914, 3, 2941],\n [925, 74, 821, 2, 1, 967, 8, 2942, 2943, 74],\n [185, 5, 240, 1075, 3, 8, 36, 41, 1708, 5, 2944, 1437, 917, 5],\n [2945, 540, 3, 7, 247, 435, 5, 889, 38, 74, 1133, 82, 4],\n [238, 78, 1, 115, 2, 2946, 11, 1, 821, 1915, 2, 889],\n [540, 1916, 4, 1914, 5, 1, 967, 3, 74, 297, 4],\n [878, 1, 73, 1410, 15, 1, 388, 1438, 2, 2947],\n [1156, 2948, 11, 1, 107, 2, 520],\n [],\n [38, 2949, 101, 21, 814, 436, 35, 167, 2950, 684],\n [45, 70, 9, 41, 2951, 2952, 3, 2953, 2954, 3, 2955, 1],\n [626, 2, 1, 167, 1917, 2, 2956, 34, 1, 2957, 2, 1, 1918],\n [8, 613, 45, 38, 74, 1133, 17, 113, 4, 1154, 1156, 26, 4],\n [547, 1, 269, 282, 5, 1, 381, 2, 1156, 57, 60, 17, 434],\n [4, 1289, 1157, 2, 1, 2958, 5, 1, 246, 2959, 15, 1, 18],\n [113, 4, 443, 4, 1154, 32, 1918, 3, 822, 2960, 9, 5, 436, 15],\n [440, 1797, 3, 238, 9, 45, 70, 39, 6, 545, 6, 17, 765, 26, 15],\n [1, 18, 4, 658, 28, 66, 364, 21, 1439, 1, 177, 97],\n [3, 73, 282, 3, 82, 4, 2961, 28, 364, 55, 8],\n [968, 6, 765, 14, 6, 1, 460, 46, 16, 57, 434],\n [4, 2962, 80, 1440, 12, 1919, 32, 99, 3, 1388, 9],\n [55, 30, 446],\n [],\n [158, 318, 2, 1, 18, 6, 1, 77, 2, 2963, 3, 886, 25],\n [42, 469, 4, 656, 428, 55, 1, 263, 2, 161, 3],\n [1920, 49, 50, 113, 1896, 14, 99, 21, 2964, 1, 18, 260, 34],\n [1827, 36, 6, 61, 90, 8, 6, 44, 79, 1921, 26],\n [881, 1, 18, 101, 656, 28, 364, 3, 82, 55, 1922],\n [202, 3, 627, 337, 87, 4, 284, 2, 47, 39, 37, 215],\n [3, 47, 39, 200, 4, 60, 3, 1, 195, 6, 412, 179, 245, 15],\n [611, 6, 1, 22, 2, 7, 962, 198, 280, 18, 9, 6, 17, 791, 22],\n [9, 6, 1, 310, 22, 215, 1309, 1, 137, 42, 26, 9, 1441],\n [1, 246, 269, 2, 16, 57, 29, 59, 383, 38, 120, 59],\n [1398, 94, 1, 823, 73, 2, 31, 23, 70, 192, 824],\n [5, 1923, 1, 85, 165, 2, 8, 185, 308, 5, 14, 112, 1],\n [1261, 6, 2965, 287, 78, 34, 1, 2966, 5, 32, 666, 112, 50, 193, 1],\n [1442, 1924, 2, 2967, 2968, 759, 10, 2969, 2970],\n [34, 216, 128, 49, 1, 18, 120, 59, 1925, 4, 260, 34, 2971, 14, 304],\n [2, 257, 389, 21, 389, 39, 105, 13, 1919, 28, 318, 234, 79],\n [1901, 9, 26, 1, 18, 41, 151, 816, 4, 583, 1926, 35, 648],\n [4, 2972, 118, 1443, 10, 1, 40, 1101, 4, 1737, 44, 1926, 3],\n [969, 48, 1, 73, 2, 28, 22, 82, 39, 1927, 648, 24],\n [360],\n [],\n [718, 39, 1927, 250, 24, 1928, 360, 1313, 3, 205],\n [39, 62, 250, 7, 218, 244, 1444, 28, 66, 825],\n [217, 26, 935, 21, 1158, 115, 2, 359, 648, 1, 195, 41],\n [7, 1784, 1319, 7, 1929, 2, 867, 33, 219, 11, 9, 549, 9, 798, 4, 612],\n [4, 29, 11, 81, 1445, 778, 11, 1, 22, 2, 32, 468, 143, 35, 1, 129],\n [106, 30, 648, 20, 5, 51, 1905, 171, 2973, 17, 153, 554],\n [],\n [49, 27, 1288, 2974, 1, 552, 16, 20, 575, 5, 1, 19],\n [27, 53, 1077, 58, 187, 343, 1930, 36, 6, 1, 31, 318, 2],\n [1, 42, 11, 1446, 5, 559, 276, 2975, 3],\n [883, 27, 25, 427, 109, 445, 1422, 1, 137, 18, 6, 35],\n [1, 505, 2, 343, 12, 561, 49, 54, 136, 197, 6, 373, 78, 49, 39, 1931],\n [447, 35, 25, 9, 6, 1407, 29, 1443, 8, 116, 12, 2976, 2977, 12, 2978, 545],\n [612, 94, 8, 116, 28, 1447, 6, 17, 247, 32, 117, 71, 208],\n ...]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_context_size+1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.739185Z",
          "iopub.execute_input": "2024-10-08T22:33:40.739855Z",
          "iopub.status.idle": "2024-10-08T22:33:40.748459Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.739795Z",
          "shell.execute_reply": "2024-10-08T22:33:40.747176Z"
        },
        "trusted": true,
        "id": "FmvyHpsIlLNk",
        "outputId": "7e9f81c9-299c-4625-94b4-37c2933f0385"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "14"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences_train, tokenized_sentences_val, _, _ = train_test_split(tokenized_sentences, tokenized_sentences, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.749933Z",
          "iopub.execute_input": "2024-10-08T22:33:40.750362Z",
          "iopub.status.idle": "2024-10-08T22:33:40.762605Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.750298Z",
          "shell.execute_reply": "2024-10-08T22:33:40.761425Z"
        },
        "trusted": true,
        "id": "Tcyi076JlLNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok_sent = []\n",
        "\n",
        "for sent in tokenized_sentences_train:\n",
        "    # si la secuencia tiene más términos que el tamaño de contexto máximo,\n",
        "    # armo varias sub-secuencias de tamaño máximo\n",
        "    if len(sent) > (max_context_size+1):\n",
        "        extra = len(sent)-(max_context_size+1) + 1\n",
        "        for i in range(extra):\n",
        "            tok_sent.append(sent[i:i+max_context_size+1])\n",
        "        else: # si la secuencia tiene menos términos el tamaño de contexto máximo, dejo la secuencia como está\n",
        "            tok_sent.append(sent)\n",
        "\n",
        "len(tok_sent)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.763912Z",
          "iopub.execute_input": "2024-10-08T22:33:40.764303Z",
          "iopub.status.idle": "2024-10-08T22:33:40.780820Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.764262Z",
          "shell.execute_reply": "2024-10-08T22:33:40.779569Z"
        },
        "trusted": true,
        "id": "iScJwMZRlLNk",
        "outputId": "7d3963fc-6a70-49a7-9683-30d17801960d"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "398"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok_sent_augm = []\n",
        "\n",
        "for sent in tok_sent:\n",
        "\n",
        "  # generamos todas las sub-secuencias\n",
        "  subseq = [sent[:i+2] for i in range(len(sent)-1)]\n",
        "  # en esta línea paddeamos al tamaño de contexto máximo\n",
        "  tok_sent_augm.append(pad_sequences(subseq, maxlen=max_context_size+1, padding='pre'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.782408Z",
          "iopub.execute_input": "2024-10-08T22:33:40.783075Z",
          "iopub.status.idle": "2024-10-08T22:33:40.826473Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.783028Z",
          "shell.execute_reply": "2024-10-08T22:33:40.825368Z"
        },
        "trusted": true,
        "id": "fbljsR7SlLNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finalmente concatenamos todas las secuencias en un único array de numpy\n",
        "train_seqs = np.concatenate(tok_sent_augm, axis=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.827871Z",
          "iopub.execute_input": "2024-10-08T22:33:40.828255Z",
          "iopub.status.idle": "2024-10-08T22:33:40.834136Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.828214Z",
          "shell.execute_reply": "2024-10-08T22:33:40.832985Z"
        },
        "trusted": true,
        "id": "floFr3WBlLNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_seqs.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.835922Z",
          "iopub.execute_input": "2024-10-08T22:33:40.836422Z",
          "iopub.status.idle": "2024-10-08T22:33:40.846308Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.836368Z",
          "shell.execute_reply": "2024-10-08T22:33:40.844993Z"
        },
        "trusted": true,
        "id": "i3E4cDx4lLNl",
        "outputId": "acb814f9-b6c5-489b-adab-f7825fd35d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(5332, 14)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y de aquí sacamos las entradas y los targets que consumirá nuestro sistema en\n",
        "# tiempo de entrenamiento\n",
        "X = train_seqs[:,:-1]\n",
        "y = train_seqs[:,-1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.848034Z",
          "iopub.execute_input": "2024-10-08T22:33:40.848553Z",
          "iopub.status.idle": "2024-10-08T22:33:40.857732Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.848498Z",
          "shell.execute_reply": "2024-10-08T22:33:40.856598Z"
        },
        "trusted": true,
        "id": "AXCx2JgilLNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Palabras del vocabulario\n",
        "tok.index_word"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.859306Z",
          "iopub.execute_input": "2024-10-08T22:33:40.859811Z",
          "iopub.status.idle": "2024-10-08T22:33:40.909937Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.859758Z",
          "shell.execute_reply": "2024-10-08T22:33:40.908763Z"
        },
        "trusted": true,
        "id": "SzhE7VNllLNl",
        "outputId": "b60b06a9-bffc-47e6-a20a-2d9c984d12f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{1: 'the',\n 2: 'of',\n 3: 'and',\n 4: 'to',\n 5: 'in',\n 6: 'is',\n 7: 'a',\n 8: 'that',\n 9: 'it',\n 10: 'with',\n 11: 'as',\n 12: 'or',\n 13: 'be',\n 14: 'this',\n 15: 'for',\n 16: 'which',\n 17: 'not',\n 18: 'child',\n 19: 'school',\n 20: 'are',\n 21: 'by',\n 22: 'work',\n 23: 'life',\n 24: 'from',\n 25: 'all',\n 26: 'but',\n 27: 'we',\n 28: 'his',\n 29: 'have',\n 30: 'its',\n 31: 'social',\n 32: 'an',\n 33: 'so',\n 34: 'on',\n 35: 'at',\n 36: 'there',\n 37: 'has',\n 38: 'they',\n 39: 'he',\n 40: 'one',\n 41: 'was',\n 42: 'children',\n 43: 'these',\n 44: 'more',\n 45: 'out',\n 46: 'upon',\n 47: 'what',\n 48: 'into',\n 49: 'if',\n 50: 'you',\n 51: 'their',\n 52: 'project',\n 53: 'may',\n 54: 'any',\n 55: 'through',\n 56: 'only',\n 57: 'i',\n 58: 'them',\n 59: 'been',\n 60: 'do',\n 61: 'no',\n 62: 'made',\n 63: 'can',\n 64: 'other',\n 65: 'will',\n 66: 'own',\n 67: 'education',\n 68: 'some',\n 69: 'our',\n 70: 'when',\n 71: 'must',\n 72: 'such',\n 73: 'conditions',\n 74: 'were',\n 75: 'gutenberg™',\n 76: '1',\n 77: 'use',\n 78: 'up',\n 79: 'than',\n 80: 'between',\n 81: 'much',\n 82: 'then',\n 83: 'things',\n 84: 'something',\n 85: 'child’s',\n 86: 'mind',\n 87: 'him',\n 88: 'occupations',\n 89: 'itself',\n 90: 'growth',\n 91: 'present',\n 92: 'make',\n 93: 'most',\n 94: 'about',\n 95: 'certain',\n 96: 'educational',\n 97: 'materials',\n 98: 'place',\n 99: 'interest',\n 100: 'history',\n 101: 'first',\n 102: 'even',\n 103: 'where',\n 104: 'attention',\n 105: 'would',\n 106: 'time',\n 107: 'material',\n 108: 'activities',\n 109: 'how',\n 110: 'world',\n 111: 'great',\n 112: 'way',\n 113: 'simply',\n 114: 'discipline',\n 115: 'study',\n 116: '”',\n 117: 'experience',\n 118: 'those',\n 119: 'etc',\n 120: 'had',\n 121: 'problem',\n 122: 'gutenberg',\n 123: 'shall',\n 124: 'intellectual',\n 125: 'should',\n 126: 'who',\n 127: 'nature',\n 128: 'side',\n 129: 'same',\n 130: 'give',\n 131: 'order',\n 132: 'because',\n 133: 'natural',\n 134: 'get',\n 135: 'works',\n 136: 'new',\n 137: 'little',\n 138: 'point',\n 139: 'thus',\n 140: 'means',\n 141: 'information',\n 142: 'connection',\n 143: 'while',\n 144: 'university',\n 145: 'part',\n 146: 'results',\n 147: 'whole',\n 148: 'matter',\n 149: 'ideas',\n 150: 'relation',\n 151: 'now',\n 152: 'activity',\n 153: 'mere',\n 154: 'years',\n 155: 'without',\n 156: 'home',\n 157: 'learning',\n 158: 'another',\n 159: 'society',\n 160: 'many',\n 161: 'form',\n 162: 'physical',\n 163: 'full',\n 164: 'also',\n 165: 'idea',\n 166: 'sense',\n 167: 'various',\n 168: 'terms',\n 169: 'elementary',\n 170: 'necessary',\n 171: 'possible',\n 172: 'individual',\n 173: 'science',\n 174: 'instead',\n 175: 'say',\n 176: 'studies',\n 177: 'facts',\n 178: 'direct',\n 179: 'different',\n 180: 'play',\n 181: 'electronic',\n 182: 'development',\n 183: 'find',\n 184: 'just',\n 185: 'found',\n 186: 'states',\n 187: 'under',\n 188: 'system',\n 189: 'processes',\n 190: 'training',\n 191: 'practical',\n 192: 'people',\n 193: 'see',\n 194: 'course',\n 195: 'result',\n 196: 'art',\n 197: 'subject',\n 198: 'year',\n 199: 'yet',\n 200: 'needs',\n 201: 'being',\n 202: 'question',\n 203: 'need',\n 204: 'two',\n 205: 'imagination',\n 206: 'end',\n 207: 'does',\n 208: 'come',\n 209: 'e',\n 210: 'parts',\n 211: 'aim',\n 212: 'power',\n 213: 'knowledge',\n 214: 'making',\n 215: 'done',\n 216: 'each',\n 217: 'thought',\n 218: 'free',\n 219: 'far',\n 220: 'problems',\n 221: 'foundation',\n 222: 'copyright',\n 223: 'psychology',\n 224: 'change',\n 225: 'teacher',\n 226: 'interests',\n 227: 'us',\n 228: 'general',\n 229: 'comes',\n 230: 'occupation',\n 231: 'real',\n 232: 'human',\n 233: 'taken',\n 234: 'rather',\n 235: 'principles',\n 236: 'others',\n 237: 'given',\n 238: 'take',\n 239: 'becomes',\n 240: 'both',\n 241: 'could',\n 242: 'culture',\n 243: 'language',\n 244: 'illustration',\n 245: 'here',\n 246: 'particular',\n 247: 'large',\n 248: 'back',\n 249: 'necessity',\n 250: 'again',\n 251: 'methods',\n 252: 'basis',\n 253: 'kindergarten',\n 254: 'united',\n 255: 'before',\n 256: 'schools',\n 257: 'thing',\n 258: 'living',\n 259: 'forms',\n 260: 'go',\n 261: 'often',\n 262: 'action',\n 263: 'medium',\n 264: 'contact',\n 265: 'having',\n 266: 'attitude',\n 267: 'man',\n 268: 'very',\n 269: 'principle',\n 270: 'period',\n 271: 'business',\n 272: 'past',\n 273: 'mental',\n 274: 'agreement',\n 275: 'license',\n 276: 'personal',\n 277: 'method',\n 278: 'larger',\n 279: 'typical',\n 280: 'old',\n 281: 'carried',\n 282: 'involved',\n 283: 'hand',\n 284: 'consciousness',\n 285: 'active',\n 286: 'hence',\n 287: 'set',\n 288: 'spirit',\n 289: 'meaning',\n 290: 'literary',\n 291: 'second',\n 292: '3',\n 293: 'less',\n 294: 'geography',\n 295: 'over',\n 296: 'since',\n 297: 'ready',\n 298: 'hardly',\n 299: 'getting',\n 300: 'giving',\n 301: 'unity',\n 302: 'become',\n 303: 'value',\n 304: 'sort',\n 305: 'instruction',\n 306: 'technical',\n 307: 'resources',\n 308: 'expression',\n 309: 'well',\n 310: 'best',\n 311: 'put',\n 312: 'merely',\n 313: 'earth',\n 314: 'lines',\n 315: 'every',\n 316: 'cannot',\n 317: 'lessons',\n 318: 'instinct',\n 319: 'variety',\n 320: 'fact',\n 321: 'questions',\n 322: 'inquiry',\n 323: 'already',\n 324: 'primary',\n 325: 'progress',\n 326: 'spinning',\n 327: 'enough',\n 328: 'believe',\n 329: 'standpoint',\n 330: 'said',\n 331: 'going',\n 332: 'after',\n 333: 'industrial',\n 334: 'within',\n 335: 'always',\n 336: 'constructive',\n 337: 'bring',\n 338: 'hold',\n 339: 'together',\n 340: 'historical',\n 341: 'scientific',\n 342: 'used',\n 343: 'four',\n 344: 'definite',\n 345: 'secure',\n 346: 'donations',\n 347: 'copy',\n 348: 'country',\n 349: 'archive',\n 350: 'drawing',\n 351: 'long',\n 352: 'ideal',\n 353: 'true',\n 354: 'too',\n 355: 'light',\n 356: 'object',\n 357: 'important',\n 358: 'complete',\n 359: 'actual',\n 360: 'observation',\n 361: 'powers',\n 362: 'makes',\n 363: 'reference',\n 364: 'impulse',\n 365: 'environment',\n 366: 'himself',\n 367: 'practice',\n 368: 'she',\n 369: 'external',\n 370: 'trademark',\n 371: 'ebook',\n 372: 'character',\n 373: 'brought',\n 374: 'future',\n 375: 'curriculum',\n 376: 'forces',\n 377: 'process',\n 378: 'reality',\n 379: 'number',\n 380: 'motive',\n 381: 'cooking',\n 382: 'later',\n 383: 'speaking',\n 384: 'working',\n 385: 'common',\n 386: 'organization',\n 387: 'psychological',\n 388: 'proper',\n 389: 'day',\n 390: 'phases',\n 391: 'access',\n 392: 'special',\n 393: 'whether',\n 394: 'theory',\n 395: 'tools',\n 396: 'outside',\n 397: 'lives',\n 398: 'following',\n 399: 'forth',\n 400: 'laws',\n 401: 'using',\n 402: 'chicago',\n 403: 's',\n 404: 'waste',\n 405: 'opportunity',\n 406: 'co',\n 407: 'paragraph',\n 408: 'view',\n 409: 'community',\n 410: 'your',\n 411: 'isolated',\n 412: 'quite',\n 413: 'ends',\n 414: 'household',\n 415: 'factors',\n 416: 'really',\n 417: 'needed',\n 418: 'sake',\n 419: 'introduction',\n 420: 'called',\n 421: 'agencies',\n 422: 'aims',\n 423: 'experiences',\n 424: 'books',\n 425: 'doing',\n 426: 'directed',\n 427: 'know',\n 428: 'themselves',\n 429: 'live',\n 430: 'instincts',\n 431: 'deal',\n 432: 'control',\n 433: 'regarded',\n 434: 'wish',\n 435: 'amount',\n 436: 'water',\n 437: 'plays',\n 438: 'almost',\n 439: 'book',\n 440: 'three',\n 441: 'partly',\n 442: 'perhaps',\n 443: 'desire',\n 444: 'normal',\n 445: 'self',\n 446: 'direction',\n 447: 'anything',\n 448: 'growing',\n 449: 'modes',\n 450: 'dealing',\n 451: 'indeed',\n 452: 'practically',\n 453: 'center',\n 454: 'familiar',\n 455: 'building',\n 456: 'remote',\n 457: 'next',\n 458: 'significance',\n 459: 'ways',\n 460: 'difference',\n 461: 'help',\n 462: 'traditional',\n 463: 'keep',\n 464: 'cotton',\n 465: 'story',\n 466: 'laboratory',\n 467: 'college',\n 468: 'adult',\n 469: 'like',\n 470: 'down',\n 471: 'represent',\n 472: 'think',\n 473: 'chart',\n 474: 'however',\n 475: 'account',\n 476: '2',\n 477: 'including',\n 478: 'f',\n 479: 'www',\n 480: 'changes',\n 481: 'statement',\n 482: 'detail',\n 483: 'until',\n 484: 'easy',\n 485: 'presented',\n 486: 'naturally',\n 487: 'want',\n 488: 'effort',\n 489: 'evolution',\n 490: 'gives',\n 491: 'manual',\n 492: 'production',\n 493: 'wool',\n 494: 'entire',\n 495: 'continual',\n 496: 'weaving',\n 497: 'daily',\n 498: 'purpose',\n 499: 'relationship',\n 500: 'insight',\n 501: 'suggestions',\n 502: 'abstract',\n 503: 'historic',\n 504: 'intelligent',\n 505: 'age',\n 506: 'did',\n 507: 'unless',\n 508: 'higher',\n 509: 'room',\n 510: 'except',\n 511: 'limited',\n 512: 'relations',\n 513: 'agree',\n 514: 'org',\n 515: 'u',\n 516: 'my',\n 517: 'clear',\n 518: 'whatever',\n 519: 'ago',\n 520: 'experiment',\n 521: 'acquaintance',\n 522: 'read',\n 523: 'better',\n 524: 'nothing',\n 525: 'teachers',\n 526: 'industry',\n 527: 'supply',\n 528: 'distribution',\n 529: 'immediate',\n 530: 'reading',\n 531: 'vital',\n 532: 'reasons',\n 533: 'spontaneous',\n 534: 'capable',\n 535: 'directly',\n 536: 'bringing',\n 537: 'learn',\n 538: 'along',\n 539: 'demand',\n 540: 'food',\n 541: 'construction',\n 542: 'type',\n 543: 'fundamental',\n 544: 'points',\n 545: 'told',\n 546: 'instance',\n 547: 'understand',\n 548: 'seeing',\n 549: 'goes',\n 550: 'tendency',\n 551: 'certainly',\n 552: 'impulses',\n 553: 'high',\n 554: 'symbols',\n 555: 'purely',\n 556: 'appeal',\n 557: 'might',\n 558: 'difficulty',\n 559: 'conversation',\n 560: 'selected',\n 561: 'five',\n 562: 'textile',\n 563: 'teach',\n 564: 'taught',\n 565: 'state',\n 566: 'isolation',\n 567: 'games',\n 568: 'grammar',\n 569: 'everyday',\n 570: 'carry',\n 571: 'paid',\n 572: 'refund',\n 573: 'away',\n 574: 'located',\n 575: 'available',\n 576: 'therefore',\n 577: 'indicated',\n 578: 'engaged',\n 579: 'satisfaction',\n 580: 'coming',\n 581: 'latter',\n 582: 'city',\n 583: 'look',\n 584: 'especially',\n 585: 'conceive',\n 586: 'connect',\n 587: 'appear',\n 588: 'application',\n 589: 'moral',\n 590: 'lies',\n 591: 'neighborhood',\n 592: 'today',\n 593: 'open',\n 594: 'gained',\n 595: 'kind',\n 596: 'uses',\n 597: 'acquired',\n 598: 'afford',\n 599: 'days',\n 600: 'commercial',\n 601: 'sewing',\n 602: 'reason',\n 603: 'preparation',\n 604: 'obvious',\n 605: 'thoroughly',\n 606: 'gets',\n 607: 'distributing',\n 608: 'skill',\n 609: 'led',\n 610: 'twelve',\n 611: 'example',\n 612: 'me',\n 613: 'worked',\n 614: 'either',\n 615: 'few',\n 616: 'considerable',\n 617: 'command',\n 618: 'outward',\n 619: 'secondary',\n 620: 'dominant',\n 621: 'presentation',\n 622: 'symbolic',\n 623: 'involves',\n 624: 'right',\n 625: 'experimental',\n 626: 'effect',\n 627: 'suggestion',\n 628: 'permanent',\n 629: 'beginning',\n 630: 'represented',\n 631: 'attempt',\n 632: 'subjects',\n 633: 'law',\n 634: 'earlier',\n 635: 'men',\n 636: 'organic',\n 637: 'collection',\n 638: 'furnish',\n 639: 'continuity',\n 640: 'program',\n 641: 'provide',\n 642: 'associated',\n 643: 'fee',\n 644: 'press',\n 645: 'start',\n 646: 'due',\n 647: 'froebel’s',\n 648: 'trees',\n 649: 'interested',\n 650: 'word',\n 651: 'thoughts',\n 652: 'mode',\n 653: 'recognize',\n 654: 'movement',\n 655: 'operation',\n 656: 'express',\n 657: 'narrow',\n 658: 'realize',\n 659: 'discussion',\n 660: 'plane',\n 661: 'devices',\n 662: 'changed',\n 663: 'affair',\n 664: 'least',\n 665: 'turn',\n 666: 'impossible',\n 667: 'ground',\n 668: 'name',\n 669: 'regarding',\n 670: 'centers',\n 671: 'century',\n 672: 'truths',\n 673: 'formal',\n 674: 'clothing',\n 675: 'followed',\n 676: 'effective',\n 677: 'purposes',\n 678: 'got',\n 679: 'plants',\n 680: 'introduced',\n 681: 'good',\n 682: 'realities',\n 683: 'toward',\n 684: 'finding',\n 685: 'takes',\n 686: 'still',\n 687: 'sometimes',\n 688: 'schoolroom',\n 689: 'words',\n 690: 'measure',\n 691: 'recitation',\n 692: 'ordinary',\n 693: 'motives',\n 694: 'learns',\n 695: 'continuous',\n 696: 'source',\n 697: 'emotional',\n 698: 'interpretation',\n 699: 'developed',\n 700: 'follow',\n 701: 'mechanical',\n 702: 'fibers',\n 703: 'why',\n 704: 'third',\n 705: 'speak',\n 706: 'throughout',\n 707: 'case',\n 708: 'stand',\n 709: 'term',\n 710: 'truth',\n 711: 'learned',\n 712: 'kinds',\n 713: 'ideals',\n 714: 'utility',\n 715: 'simple',\n 716: 'writing',\n 717: 'conscious',\n 718: 'finally',\n 719: 'recognized',\n 720: 'desirable',\n 721: 'organized',\n 722: 'continually',\n 723: 'moreover',\n 724: 'eight',\n 725: 'equipment',\n 726: 'provided',\n 727: 'chief',\n 728: 'represents',\n 729: 'vivid',\n 730: 'body',\n 731: 'solution',\n 732: 'ages',\n 733: 'once',\n 734: 'studied',\n 735: 'union',\n 736: 'extending',\n 737: 'library',\n 738: 'model',\n 739: 'beyond',\n 740: 'sufficient',\n 741: 'froebel',\n 742: 'copies',\n 743: 'ebooks',\n 744: 'section',\n 745: '5',\n 746: 'original',\n 747: 'hands',\n 748: 'persons',\n 749: 'transformation',\n 750: 'record',\n 751: 'influence',\n 752: 'habits',\n 753: 'range',\n 754: 'tradition',\n 755: 'minds',\n 756: 'pupils',\n 757: 'features',\n 758: 'main',\n 759: 'line',\n 760: 'modern',\n 761: 'length',\n 762: 'revolution',\n 763: 'raw',\n 764: 'gradually',\n 765: 'educative',\n 766: 'garden',\n 767: 'judgment',\n 768: 'children’s',\n 769: 'radical',\n 770: 'adaptation',\n 771: 'mean',\n 772: 'helpful',\n 773: 'talk',\n 774: 'wood',\n 775: 'types',\n 776: 'apart',\n 777: 'sympathetic',\n 778: 'feeling',\n 779: 'element',\n 780: 'ethical',\n 781: 'kitchen',\n 782: 'energy',\n 783: 'marked',\n 784: 'fixed',\n 785: 'conception',\n 786: 'difficult',\n 787: 'mother',\n 788: 'arise',\n 789: 'elements',\n 790: 'realization',\n 791: 'average',\n 792: 'fiber',\n 793: 'person',\n 794: 'race',\n 795: 'values',\n 796: 'applied',\n 797: 'eye',\n 798: 'seems',\n 799: 'moving',\n 800: 'regards',\n 801: 'stimuli',\n 802: 'highly',\n 803: 'professional',\n 804: 'leave',\n 805: 'appreciate',\n 806: 'instruments',\n 807: 'concrete',\n 808: 'bare',\n 809: 'studying',\n 810: 'her',\n 811: 'demands',\n 812: 'habit',\n 813: 'else',\n 814: 'taking',\n 815: 'valuable',\n 816: 'asked',\n 817: 'against',\n 818: 'corners',\n 819: 'older',\n 820: 'experimentation',\n 821: 'characteristic',\n 822: 'accordingly',\n 823: 'primitive',\n 824: 'lived',\n 825: 'imaginative',\n 826: 'return',\n 827: 'connected',\n 828: 'depends',\n 829: 'series',\n 830: 'objects',\n 831: 'pedagogical',\n 832: 'lack',\n 833: 'money',\n 834: 'middle',\n 835: 'lower',\n 836: 'supposed',\n 837: 'arithmetic',\n 838: 'literature',\n 839: '4',\n 840: 'laboratories',\n 841: 'reached',\n 842: 'hope',\n 843: 'arts',\n 844: 'suppose',\n 845: 'positive',\n 846: 'freely',\n 847: 'possibility',\n 848: 'imitation',\n 849: 'step',\n 850: 'emphasis',\n 851: 'doubt',\n 852: 'protected',\n 853: 'reflective',\n 854: 'distribute',\n 855: 'permission',\n 856: 'charge',\n 857: 'comply',\n 858: 'tax',\n 859: 'anyone',\n 860: 'online',\n 861: 'distributed',\n 862: 'images',\n 863: 'edition',\n 864: 'mrs',\n 865: 'whose',\n 866: 'iii',\n 867: 'forest',\n 868: 'note',\n 869: 'lectures',\n 870: 'mr',\n 871: 'operating',\n 872: 'came',\n 873: 'am',\n 874: 'peculiar',\n 875: 'contributions',\n 876: 'members',\n 877: 'inventions',\n 878: 'consider',\n 879: 'termed',\n 880: 'inevitable',\n 881: 'let',\n 882: 'aspects',\n 883: 'communication',\n 884: 'face',\n 885: 'political',\n 886: 'paper',\n 887: 'house',\n 888: 'loom',\n 889: 'animal',\n 890: 'capacity',\n 891: 'responsibility',\n 892: 'became',\n 893: 'importance',\n 894: 'ingenuity',\n 895: 'shop',\n 896: 'operative',\n 897: 'animals',\n 898: 'alertness',\n 899: 'compared',\n 900: 'behind',\n 901: 'outcome',\n 902: 'division',\n 903: 'equally',\n 904: 'introduce',\n 905: 'factor',\n 906: 'half',\n 907: 'met',\n 908: 'atmosphere',\n 909: 'actively',\n 910: 'image',\n 911: 'gain',\n 912: 'comparison',\n 913: 'one’s',\n 914: 'setting',\n 915: 'previous',\n 916: 'regard',\n 917: 'quantity',\n 918: 'securing',\n 919: 'holding',\n 920: 'stands',\n 921: 'related',\n 922: 'dependent',\n 923: 'economic',\n 924: 'stress',\n 925: 'products',\n 926: 'effects',\n 927: 'geographical',\n 928: 'nor',\n 929: 'chronological',\n 930: 'supplies',\n 931: 'brings',\n 932: 'teaching',\n 933: 'during',\n 934: 'directions',\n 935: 'controlled',\n 936: 'position',\n 937: 'required',\n 938: 'able',\n 939: 'becoming',\n 940: 'hear',\n 941: 'liberal',\n 942: 'appeals',\n 943: 'aspect',\n 944: 'separation',\n 945: 'grade',\n 946: 'whom',\n 947: 'adequate',\n 948: 'concerned',\n 949: 'providing',\n 950: 'worthy',\n 951: 'last',\n 952: 'week',\n 953: 'everything',\n 954: 'distinctive',\n 955: 'ear',\n 956: 'capacities',\n 957: 'six',\n 958: 'previously',\n 959: 'lesson',\n 960: 'primarily',\n 961: 'contrast',\n 962: 'seven',\n 963: 'plan',\n 964: 'plenty',\n 965: 'hard',\n 966: 'transition',\n 967: 'vegetables',\n 968: 'recognition',\n 969: 'consciously',\n 970: 'sufficiently',\n 971: 'finds',\n 972: 'outlet',\n 973: 'investigation',\n 974: 'advantage',\n 975: 'took',\n 976: 'keeping',\n 977: 'capital',\n 978: 'particularly',\n 979: 'utilize',\n 980: 'appropriate',\n 981: 'serious',\n 982: 'sorts',\n 983: 'indicate',\n 984: 'creating',\n 985: 'matters',\n 986: 'interaction',\n 987: 'emphasized',\n 988: 'grew',\n 989: 'department',\n 990: 'grades',\n 991: 'sequence',\n 992: 'artificial',\n 993: 'soil',\n 994: 'existing',\n 995: 'balance',\n 996: 'local',\n 997: 'bear',\n 998: 'wheel',\n 999: 'spiritual',\n 1000: 'bound',\n ...}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cantidad de palabras en el vocabulario\n",
        "vocab_size = len(tok.word_counts)\n",
        "vocab_size"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.911687Z",
          "iopub.execute_input": "2024-10-08T22:33:40.912759Z",
          "iopub.status.idle": "2024-10-08T22:33:40.920076Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.912693Z",
          "shell.execute_reply": "2024-10-08T22:33:40.918798Z"
        },
        "trusted": true,
        "id": "M7NuJsrZlLNl",
        "outputId": "113eabc3-1ccf-406c-da28-ef8013895379"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "4504"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definición del modelo"
      ],
      "metadata": {
        "id": "3uX_aF3TlLNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el modelo de prueba se diseño de la siguiente manera:\n",
        "\n",
        "Una capa de embedding\n",
        "\n",
        "Dos capas bidireccionales (LSTM) para que la red procese las secuencias en ambas\n",
        "\n",
        "Tres capas densas"
      ],
      "metadata": {
        "id": "kbuN_ImNlLNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Embedding\n",
        "model.add(Embedding(input_dim=vocab_size + 1, output_dim=5, input_shape=(max_context_size,)))\n",
        "\n",
        "# LSTM Bidireccional\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(64))) # La última capa LSTM no lleva return_sequences\n",
        "\n",
        "# Agregar más capas densas\n",
        "model.add(Dense(64, activation='relu'))  # Aumentar neuronas\n",
        "model.add(Dense(32, activation='relu'))  # Otra capa densa\n",
        "\n",
        "# Predicción de clasificación con softmax\n",
        "model.add(Dense(vocab_size + 1, activation='softmax'))\n",
        "\n",
        "# Compilación del modelo\n",
        "model.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam')\n",
        "\n",
        "# Resumen del modelo\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:40.921492Z",
          "iopub.execute_input": "2024-10-08T22:33:40.921889Z",
          "iopub.status.idle": "2024-10-08T22:33:41.228051Z",
          "shell.execute_reply.started": "2024-10-08T22:33:40.921849Z",
          "shell.execute_reply": "2024-10-08T22:33:41.226855Z"
        },
        "trusted": true,
        "id": "DjSS5NzllLNl",
        "outputId": "411b33db-ccda-47cd-bb0e-a0f7fd08ea3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mModel: \"sequential\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m5\u001b[0m)          │        \u001b[38;5;34m22,525\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m35,840\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4505\u001b[0m)           │       \u001b[38;5;34m148,665\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,525</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">35,840</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4505</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">148,665</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m316,182\u001b[0m (1.21 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">316,182</span> (1.21 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m316,182\u001b[0m (1.21 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">316,182</span> (1.21 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PplCallback(keras.callbacks.Callback):\n",
        "    ''''\n",
        "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
        "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, val_data):\n",
        "        # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
        "        # mediremos la perplejidad\n",
        "        self.val_data = val_data\n",
        "\n",
        "        self.target = []\n",
        "        self.padded = []\n",
        "\n",
        "        count = 0\n",
        "        self.info = []\n",
        "\n",
        "        # nos movemos en todas las secuencias de los datos de validación\n",
        "        for seq in self.val_data:\n",
        "            len_seq = len(seq)\n",
        "            # armamos todas las subsecuencias\n",
        "            subseq = [seq[:i] for i in range(len_seq)]\n",
        "            self.target.extend([seq[i] for i in range(len_seq)])\n",
        "            self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
        "\n",
        "            self.info.append((count,count+len_seq))\n",
        "            count += len_seq\n",
        "\n",
        "        self.padded = np.vstack(self.padded)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        scores = []\n",
        "        predictions = self.model.predict(self.padded, verbose=0)\n",
        "\n",
        "        for start, end in self.info:\n",
        "            probs = [predictions[idx_seq, idx_vocab] + epsilon for idx_seq, idx_vocab in zip(range(start, end), self.target[start:end])]\n",
        "            if any(prob <= 0 for prob in probs):  # Verifica si hay probabilidades <= 0\n",
        "                print(\"Warning: Found probability <= 0, may cause NaN in perplexity.\")\n",
        "\n",
        "        scores.append(np.exp(-np.sum(np.log(probs)) / (end - start)))\n",
        "\n",
        "        print(f'\\n mean perplexity: {np.mean(scores)} \\n')\n",
        "\n",
        " #   def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        " #       # en `scores` iremos guardando la perplejidad de cada secuencia\n",
        " #       scores = []\n",
        "\n",
        " #       predictions = self.model.predict(self.padded,verbose=0)\n",
        "\n",
        " #       # para cada secuencia de validación\n",
        " #       for start,end in self.info:\n",
        "\n",
        " #         # en `probs` iremos guardando las probabilidades de los términos target\n",
        " #         probs = [predictions[idx_seq,idx_vocab] for idx_seq, idx_vocab in zip(range(start,end),self.target[start:end])]\n",
        "\n",
        " #         # calculamos la perplejidad por medio de logaritmos\n",
        " #         scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
        "\n",
        " #       # promediamos todos los scores e imprimimos el valor promedio\n",
        " #       print(f'\\n mean perplexity: {np.mean(scores)} \\n')\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:41.229762Z",
          "iopub.execute_input": "2024-10-08T22:33:41.230200Z",
          "iopub.status.idle": "2024-10-08T22:33:41.246651Z",
          "shell.execute_reply.started": "2024-10-08T22:33:41.230159Z",
          "shell.execute_reply": "2024-10-08T22:33:41.245077Z"
        },
        "trusted": true,
        "id": "2F5mVGmSlLNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 1e-10\n",
        "# fiteamos, nótese el agregado del callback con su inicialización. El batch_size lo podemos seleccionar a mano\n",
        "# en general, mientras más grande mejor.\n",
        "hist = model.fit(X, y, epochs=75, callbacks=[PplCallback(tokenized_sentences_val)], batch_size=32)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:33:41.248168Z",
          "iopub.execute_input": "2024-10-08T22:33:41.248981Z",
          "iopub.status.idle": "2024-10-08T22:41:21.019884Z",
          "shell.execute_reply.started": "2024-10-08T22:33:41.248934Z",
          "shell.execute_reply": "2024-10-08T22:41:21.018635Z"
        },
        "trusted": true,
        "id": "GPmjcXLalLNm",
        "outputId": "928f60fd-69db-448f-90bc-25ba8ee919d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 6.9178\n mean perplexity: 1279.896460985434 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - loss: 6.9076\nEpoch 2/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 5.3153\n mean perplexity: 2226.6052525217738 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 5.3150\nEpoch 3/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 5.2575\n mean perplexity: 1776.604399945906 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 5.2572\nEpoch 4/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.1375\n mean perplexity: 2327.0465228435655 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 5.1370\nEpoch 5/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.9662\n mean perplexity: 5582.212233026264 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 4.9663\nEpoch 6/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 4.7356\n mean perplexity: 10567.495166447052 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - loss: 4.7357\nEpoch 7/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.6034\n mean perplexity: 18566.20268877501 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 4.6036\nEpoch 8/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.5543\n mean perplexity: 30882.95456990405 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 4.5541\nEpoch 9/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.4215\n mean perplexity: 36471.608538174005 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 4.4217\nEpoch 10/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.3465\n mean perplexity: 59680.535018171046 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 4.3466\nEpoch 11/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 4.2784\n mean perplexity: 85241.79893309122 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - loss: 4.2783\nEpoch 12/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.1478\n mean perplexity: 105781.93697633738 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 4.1479\nEpoch 13/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.0630\n mean perplexity: 164621.7333032321 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 4.0634\nEpoch 14/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.0089\n mean perplexity: 198009.13919569255 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 4.0089\nEpoch 15/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.9497\n mean perplexity: 236501.49935336984 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 3.9497\nEpoch 16/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.8351\n mean perplexity: 221152.87191353916 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 3.8357\nEpoch 17/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.7814\n mean perplexity: 252785.20541311923 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 3.7816\nEpoch 18/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.7517\n mean perplexity: 243476.10279803548 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 3.7518\nEpoch 19/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.6589\n mean perplexity: 270263.38415592106 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 3.6592\nEpoch 20/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.6017\n mean perplexity: 254856.56033458273 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 3.6018\nEpoch 21/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.5709\n mean perplexity: 327575.3054101396 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 3.5706\nEpoch 22/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 3.4416\n mean perplexity: 283910.21415602334 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 3.4420\nEpoch 23/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.3854\n mean perplexity: 267182.8923388579 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 3.3858\nEpoch 24/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.3238\n mean perplexity: 303909.87011596875 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 3.3240\nEpoch 25/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.2608\n mean perplexity: 294453.93352016655 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 3.2610\nEpoch 26/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.2214\n mean perplexity: 317983.0723728972 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 3.2215\nEpoch 27/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 3.1445\n mean perplexity: 348595.23307425715 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 3.1446\nEpoch 28/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.0772\n mean perplexity: 341361.8145934768 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 3.0775\nEpoch 29/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.0108\n mean perplexity: 374562.09524147975 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 3.0109\nEpoch 30/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.9304\n mean perplexity: 329424.0227620508 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 2.9313\nEpoch 31/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.8794\n mean perplexity: 355148.53836981364 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 2.8801\nEpoch 32/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.8088\n mean perplexity: 315598.34615306836 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - loss: 2.8097\nEpoch 33/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.7967\n mean perplexity: 369173.6374826208 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 2.7970\nEpoch 34/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.7148\n mean perplexity: 351476.0003375229 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 2.7152\nEpoch 35/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.6595\n mean perplexity: 383636.62045618816 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 2.6597\nEpoch 36/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.6449\n mean perplexity: 408248.4817645966 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 2.6453\nEpoch 37/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.5436\n mean perplexity: 317008.3394025674 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 2.5445\nEpoch 38/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.5266\n mean perplexity: 559914.4241040827 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 2.5268\nEpoch 39/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.4498\n mean perplexity: 396725.4127871704 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 2.4502\nEpoch 40/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.4557\n mean perplexity: 468428.44390135154 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 2.4561\nEpoch 41/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.3679\n mean perplexity: 541541.5328441752 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 2.3690\nEpoch 42/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.3473\n mean perplexity: 367914.89092675905 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 2.3480\nEpoch 43/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.3458\n mean perplexity: 366099.75720713625 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 2.3461\nEpoch 44/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.2740\n mean perplexity: 448782.58247019566 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 2.2742\nEpoch 45/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.2137\n mean perplexity: 531402.3504367712 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 2.2148\nEpoch 46/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.1807\n mean perplexity: 411751.98080955073 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 2.1815\nEpoch 47/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.1410\n mean perplexity: 760469.1810656453 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 2.1416\nEpoch 48/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.1502\n mean perplexity: 548349.1744665656 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 2.1502\nEpoch 49/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.0899\n mean perplexity: 619088.2408796895 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 2.0903\nEpoch 50/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.1210\n mean perplexity: 487430.70367479476 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 2.1210\nEpoch 51/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.0283\n mean perplexity: 402421.68502951413 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 2.0284\nEpoch 52/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.9875\n mean perplexity: 522935.70469494886 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 1.9877\nEpoch 53/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.9408\n mean perplexity: 781976.9720181398 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.9415\nEpoch 54/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.9271\n mean perplexity: 476101.6712793502 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 1.9275\nEpoch 55/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.8550\n mean perplexity: 647570.331061257 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.8553\nEpoch 56/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.8328\n mean perplexity: 537738.3663497437 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 1.8331\nEpoch 57/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.8378\n mean perplexity: 521298.719081379 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.8379\nEpoch 58/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.7790\n mean perplexity: 638311.4925454031 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.7792\nEpoch 59/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.7217\n mean perplexity: 529733.5557081238 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 1.7221\nEpoch 60/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.6806\n mean perplexity: 699277.3138425613 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.6811\nEpoch 61/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.7139\n mean perplexity: 816945.9929029325 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.7139\nEpoch 62/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.6442\n mean perplexity: 1052208.3691836568 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.6446\nEpoch 63/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.6038\n mean perplexity: 933237.7374677821 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.6040\nEpoch 64/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.6423\n mean perplexity: 1168459.9766996093 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.6420\nEpoch 65/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.6541\n mean perplexity: 927829.4145046691 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 1.6538\nEpoch 66/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.5112\n mean perplexity: 1049717.0613940323 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.5115\nEpoch 67/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.4922\n mean perplexity: 807903.6295561708 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.4927\nEpoch 68/75\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.4722\n mean perplexity: 659434.1976002763 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.4724\nEpoch 69/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3998\n mean perplexity: 579996.8852152135 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.4006\nEpoch 70/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.3863\n mean perplexity: 766224.843841823 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - loss: 1.3868\nEpoch 71/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3506\n mean perplexity: 796897.4859863903 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 1.3512\nEpoch 72/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3571\n mean perplexity: 684444.114822184 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.3580\nEpoch 73/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3555\n mean perplexity: 706476.0118677944 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.3558\nEpoch 74/75\n\u001b[1m166/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3059\n mean perplexity: 673079.8153058446 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.3062\nEpoch 75/75\n\u001b[1m165/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.2693\n mean perplexity: 1104767.8849625217 \n\n\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 1.2700\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T23:52:42.981060Z",
          "iopub.execute_input": "2024-10-08T23:52:42.981534Z",
          "iopub.status.idle": "2024-10-08T23:53:04.047272Z",
          "shell.execute_reply.started": "2024-10-08T23:52:42.981482Z",
          "shell.execute_reply": "2024-10-08T23:53:04.045846Z"
        },
        "trusted": true,
        "id": "dDdLWMmMlLNm",
        "outputId": "056d9ddf-3292-4776-819b-17f482bcf5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting gradio\n  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.4.0)\nRequirement already satisfied: fastapi<1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.111.0)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\nCollecting gradio-client==1.3.0 (from gradio)\n  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.27.0)\nRequirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.4.0)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.5)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.5)\nRequirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.10.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.3.0)\nRequirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.9.2)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.0.9)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.2)\nCollecting ruff>=0.2.2 (from gradio)\n  Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting tomlkit==0.12.0 (from gradio)\n  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.3)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.12.2)\nCollecting urllib3~=2.0 (from gradio)\n  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.30.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\nRequirement already satisfied: websockets<13.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio) (12.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.0)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi<1.0->gradio) (0.37.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi<1.0->gradio) (0.0.4)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi<1.0->gradio) (5.10.0)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<1.0->gradio) (2.1.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.15.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.23.4)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi<1.0->gradio) (2.6.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi<1.0->gradio) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi<1.0->gradio) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi<1.0->gradio) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi<1.0->gradio) (0.22.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\nDownloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\nInstalling collected packages: urllib3, tomlkit, semantic-version, ruff, ffmpy, gradio-client, gradio\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.18\n    Uninstalling urllib3-1.26.18:\n      Successfully uninstalled urllib3-1.26.18\n  Attempting uninstall: tomlkit\n    Found existing installation: tomlkit 0.13.2\n    Uninstalling tomlkit-0.13.2:\n      Successfully uninstalled tomlkit-0.13.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.3 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ffmpy-0.4.0 gradio-4.44.1 gradio-client-1.3.0 ruff-0.6.9 semantic-version-2.10.0 tomlkit-0.12.0 urllib3-2.2.1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T23:53:25.543372Z",
          "iopub.execute_input": "2024-10-08T23:53:25.543855Z",
          "iopub.status.idle": "2024-10-08T23:53:30.232271Z",
          "shell.execute_reply.started": "2024-10-08T23:53:25.543811Z",
          "shell.execute_reply": "2024-10-08T23:53:30.231089Z"
        },
        "trusted": true,
        "id": "_eOHQx7zlLNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_response(human_text):\n",
        "\n",
        "    # Encodeamos\n",
        "    encoded = tok.texts_to_sequences([human_text])[0]\n",
        "    # Si tienen distinto largo\n",
        "    encoded = pad_sequences([encoded], maxlen=max_context_size, padding='pre')\n",
        "\n",
        "    # Predicción softmax\n",
        "    y_hat = model.predict(encoded).argmax(axis=-1)\n",
        "\n",
        "    # Debemos buscar en el vocabulario la palabra\n",
        "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "    out_word = ''\n",
        "    for word, index in tok.word_index.items():\n",
        "        if index == y_hat:\n",
        "            out_word = word\n",
        "            break\n",
        "\n",
        "    # Agrego la palabra a la frase predicha\n",
        "    return human_text + ' ' + out_word\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=model_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\")\n",
        "\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T23:53:33.342249Z",
          "iopub.execute_input": "2024-10-08T23:53:33.343119Z"
        },
        "trusted": true,
        "id": "UouAQDxFlLNn",
        "outputId": "f31728f3-3ace-48b1-e226-23cb0e14ec8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\nRunning on public URL: https://37a6853eb2b6bade8e.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div><iframe src=\"https://37a6853eb2b6bade8e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_seq(model, tokenizer, seed_text, max_length, n_words):\n",
        "\n",
        "    \"\"\"\n",
        "        Exec model sequence prediction\n",
        "\n",
        "        Args:\n",
        "            model (keras): modelo entrenado\n",
        "            tokenizer (keras tokenizer): tonenizer utilizado en el preprocesamiento\n",
        "            seed_text (string): texto de entrada (input_seq)\n",
        "            max_length (int): máxima longitud de la sequencia de entrada\n",
        "            n_words (int): números de palabras a agregar a la sequencia de entrada\n",
        "        returns:\n",
        "            output_text (string): sentencia con las \"n_words\" agregadas\n",
        "    \"\"\"\n",
        "    output_text = seed_text\n",
        "    # generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "        # Encodeamos\n",
        "        encoded = tokenizer.texts_to_sequences([output_text])[0]\n",
        "        # Si tienen distinto largo\n",
        "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "        # Predicción softmax\n",
        "        y_hat = model.predict(encoded).argmax(axis=-1)\n",
        "        # Vamos concatenando las predicciones\n",
        "        out_word = ''\n",
        "\n",
        "        # Debemos buscar en el vocabulario la palabra\n",
        "        # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == y_hat:\n",
        "                out_word = word\n",
        "                break\n",
        "\n",
        "        # Agrego las palabras a la frase predicha\n",
        "        output_text += ' ' + out_word\n",
        "    return output_text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T23:54:12.777329Z",
          "iopub.execute_input": "2024-10-08T23:54:12.778617Z",
          "iopub.status.idle": "2024-10-08T23:54:12.813538Z",
          "shell.execute_reply.started": "2024-10-08T23:54:12.778551Z",
          "shell.execute_reply": "2024-10-08T23:54:12.812437Z"
        },
        "trusted": true,
        "id": "OkTjs4N1lLNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text='It is easy to'\n",
        "\n",
        "generate_seq(model, tok, input_text, max_length=max_context_size, n_words=2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:42:52.247557Z",
          "iopub.execute_input": "2024-10-08T22:42:52.248028Z",
          "iopub.status.idle": "2024-10-08T22:42:52.419731Z",
          "shell.execute_reply.started": "2024-10-08T22:42:52.247986Z",
          "shell.execute_reply": "2024-10-08T22:42:52.418602Z"
        },
        "trusted": true,
        "id": "88WwbjJ4lLNo",
        "outputId": "95ae2c3e-b1f9-47f3-8413-52b66e188db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
          "output_type": "stream"
        },
        {
          "execution_count": 29,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'It is easy to look at'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo está performando bien. Está creando una palabra con todo sentido. Y esta aprendiendo de la data con la que alimentamos el modelo"
      ],
      "metadata": {
        "id": "iT52zE6QlLNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# funcionalidades para hacer encoding y decoding\n",
        "\n",
        "def encode(text,max_length=max_context_size):\n",
        "\n",
        "    encoded = tok.texts_to_sequences([text])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def decode(seq):\n",
        "    return tok.sequences_to_texts([seq])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:45:17.808045Z",
          "iopub.execute_input": "2024-10-08T22:45:17.808582Z",
          "iopub.status.idle": "2024-10-08T22:45:17.815752Z",
          "shell.execute_reply.started": "2024-10-08T22:45:17.808526Z",
          "shell.execute_reply": "2024-10-08T22:45:17.814301Z"
        },
        "trusted": true,
        "id": "A0lknFF5lLNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "# función que selecciona candidatos para el beam search\n",
        "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp=1):\n",
        "    # colectar todas las probabilidades para la siguiente búsqueda\n",
        "    pred_large = []\n",
        "\n",
        "    for idx,pp in enumerate(pred):\n",
        "        pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
        "\n",
        "    pred_large = np.array(pred_large)\n",
        "\n",
        "    # criterio de selección\n",
        "    # idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
        "    idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo\n",
        "\n",
        "    # traducir a índices de token en el vocabulario\n",
        "    new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
        "                        np.array([idx_select%vocab_size]).T),\n",
        "                      axis=1)\n",
        "\n",
        "    # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
        "    return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
        "\n",
        "\n",
        "def beam_search(model,num_beams,num_words,input):\n",
        "\n",
        "    # first iteration\n",
        "\n",
        "    # encode\n",
        "    encoded = encode(input)\n",
        "\n",
        "    # first prediction\n",
        "    y_hat = np.squeeze(model.predict(encoded))\n",
        "\n",
        "    # get vocabulary size\n",
        "    vocab_size = y_hat.shape[0]\n",
        "\n",
        "    # initialize history\n",
        "    history_probs = [0]*num_beams\n",
        "    history_tokens = [encoded[0]]*num_beams\n",
        "\n",
        "    # select num_beams candidates\n",
        "    history_probs, history_tokens = select_candidates([y_hat],\n",
        "                                        num_beams,\n",
        "                                        vocab_size,\n",
        "                                        history_probs,\n",
        "                                        history_tokens)\n",
        "\n",
        "    # beam search loop\n",
        "    for i in range(num_words-1):\n",
        "        preds = []\n",
        "\n",
        "        for hist in history_tokens:\n",
        "\n",
        "            # actualizar secuencia de tokens\n",
        "            input_update = np.array([hist[i+1:]]).copy()\n",
        "\n",
        "            # predicción\n",
        "            y_hat = np.squeeze(model.predict(input_update))\n",
        "\n",
        "            preds.append(y_hat)\n",
        "\n",
        "        history_probs, history_tokens = select_candidates(preds,\n",
        "                                                        num_beams,\n",
        "                                                        vocab_size,\n",
        "                                                        history_probs,\n",
        "                                                        history_tokens)\n",
        "\n",
        "        return history_tokens"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:50:54.956174Z",
          "iopub.execute_input": "2024-10-08T22:50:54.957656Z",
          "iopub.status.idle": "2024-10-08T22:50:54.974489Z",
          "shell.execute_reply.started": "2024-10-08T22:50:54.957540Z",
          "shell.execute_reply": "2024-10-08T22:50:54.972646Z"
        },
        "trusted": true,
        "id": "4tJDSrY3lLNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicción con beam search\n",
        "salidas = beam_search(model,num_beams=25,num_words=6,input=\"when i find myself in times\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T22:51:15.104101Z",
          "iopub.execute_input": "2024-10-08T22:51:15.104611Z",
          "iopub.status.idle": "2024-10-08T22:51:17.353863Z",
          "shell.execute_reply.started": "2024-10-08T22:51:15.104562Z",
          "shell.execute_reply": "2024-10-08T22:51:17.352634Z"
        },
        "trusted": true,
        "id": "lYeuEPHblLNp",
        "outputId": "80572bc3-5f3c-492a-a461-a04cbb8db690"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# veamos las salidas\n",
        "for i in range(len(salidas)):\n",
        "    print(decode(salidas[i]))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T23:51:08.878917Z",
          "iopub.execute_input": "2024-10-08T23:51:08.879331Z",
          "iopub.status.idle": "2024-10-08T23:51:08.886542Z",
          "shell.execute_reply.started": "2024-10-08T23:51:08.879293Z",
          "shell.execute_reply": "2024-10-08T23:51:08.885187Z"
        },
        "trusted": true,
        "id": "T8MAvB3GlLNp",
        "outputId": "ac5abeef-da87-4278-87a3-5de67b6c1bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['when i find myself in times it the']\n['when i find myself in times is as']\n['when i find myself in times is to']\n['when i find myself in times that away']\n['when i find myself in times is so']\n['when i find myself in times that the']\n['when i find myself in times is a']\n['when i find myself in times is a']\n['when i find myself in times is we']\n['when i find myself in times that the']\n['when i find myself in times is to']\n['when i find myself in times is that']\n['when i find myself in times is to']\n['when i find myself in times is true']\n['when i find myself in times is we']\n['when i find myself in times as it']\n['when i find myself in times is we']\n['when i find myself in times is to']\n['when i find myself in times is “i']\n['when i find myself in times that the']\n['when i find myself in times that the']\n['when i find myself in times is by']\n['when i find myself in times is a']\n['when i find myself in times that that']\n['when i find myself in times is that']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se puede evidenciar las diversas variantes creadas por el algoritmo."
      ],
      "metadata": {
        "id": "TrB6YQnolLNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ejXf-Oh1lLNw"
      }
    }
  ]
}